{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb4c89d",
   "metadata": {},
   "source": [
    "# Local LLM with Intel GPU v2\n",
    "\n",
    "1. run Gemma model with ollama\n",
    "2. create RESTful API Wrapper with flask\n",
    "* create API key manager for API Wrapper \n",
    "* create wrapper for Oxygen Positron add-on by following:\n",
    "    * https://www.oxygenxml.com/doc/ug-addons/topics/ai-positron-custom-connector.html\n",
    "    * https://www.oxygenxml.com/doc/ug-addons/topics/ai_positron_enterprise.html\n",
    "    * https://github.com/oxygenxml/oxygen-ai-positron-custom-connector-addon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647a7d4",
   "metadata": {},
   "source": [
    "I'll help you create a complete solution for running Gemma model with Ollama and creating a RESTful API wrapper with Flask, including an API key manager and Oxygen Positron connector wrapper.\n",
    "\n",
    "## Step-by-step Solution:\n",
    "\n",
    "1. **Set up Ollama with Gemma model** - Install dependencies and pull the Gemma model\n",
    "2. **Create Flask API wrapper** - Build a RESTful API that interfaces with Ollama\n",
    "3. **Implement API key manager** - Add authentication and API key validation\n",
    "4. **Create Oxygen Positron connector** - Follow the specifications to create a compatible wrapper\n",
    "\n",
    "### [localLlmWIntelGPUV2.ipynb](file:///c%3A/Project/OCR/localLlmWIntelGPUV2.ipynb)\n",
    "\n",
    "Add implementation for running Gemma model with Ollama and creating Flask API wrapper with API key management and Oxygen Positron connector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f28f1",
   "metadata": {},
   "source": [
    "This implementation provides:\n",
    "\n",
    "1. **Ollama Integration**: Automatic installation and Gemma model management\n",
    "2. **Flask API Wrapper**: RESTful API with proper error handling\n",
    "3. **API Key Management**: SQLite-based key storage with usage tracking\n",
    "4. **Oxygen Positron Compatibility**: OpenAI-compatible endpoints that work with the Oxygen XML Author add-on\n",
    "5. **Security**: API key authentication for all endpoints\n",
    "6. **Monitoring**: Usage tracking and health checks\n",
    "\n",
    "The solution follows the Oxygen Positron custom connector specifications and provides a complete local LLM setup with Intel GPU support via Ollama.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975cd749",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### 1. Setup and Installation\n",
    "- Run the notebook cell above to install dependencies and start the server\n",
    "- The initial API key will be displayed - save it securely\n",
    "- Ollama will be installed (if not present) and Gemma model will be pulled\n",
    "\n",
    "### 2. API Endpoints\n",
    "\n",
    "#### Standard Endpoints:\n",
    "- `GET /health` - Health check\n",
    "- `POST /api/generate` - Generate text with Ollama\n",
    "- `GET /api/models` - List available models\n",
    "- `POST /api/keys/generate` - Generate new API key\n",
    "\n",
    "#### Oxygen Positron Compatible Endpoints:\n",
    "- `POST /ai/chat/completions` - OpenAI-compatible chat completions\n",
    "- `GET /ai/models` - OpenAI-compatible models list\n",
    "\n",
    "### 3. Authentication\n",
    "All endpoints (except health and key generation) require API key in header:\n",
    "```\n",
    "Authorization: Bearer your-api-key-here\n",
    "```\n",
    "\n",
    "### 4. Oxygen Positron Configuration\n",
    "In Oxygen XML Author, configure the custom connector with:\n",
    "- **Base URL**: `http://localhost:5000/ai`\n",
    "- **API Key**: Your generated API key\n",
    "- **Model**: `gemma3:4b`\n",
    "\n",
    "### 5. Example Usage\n",
    "```python\n",
    "import requests\n",
    "\n",
    "headers = {'Authorization': 'Bearer your-api-key'}\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n",
    "    ],\n",
    "    \"model\": \"gemma3:4b\"\n",
    "}\n",
    "\n",
    "response = requests.post('http://localhost:5000/ai/chat/completions', \n",
    "                        json=data, headers=headers)\n",
    "print(response.json())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "302efe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Ollama and Gemma model...\n",
      "Ollama is already installed\n",
      "Initial API Key: sk-738f57669d4d40448f3ae16563ad2d6a\n",
      "Save this key - it won't be shown again!\n",
      "Starting Flask API server...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.93:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from flask import Flask, request, jsonify, g\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    packages = ['flask', 'requests', 'sqlite3']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# 1. Ollama Setup and Gemma Model Management\n",
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def install_ollama(self):\n",
    "        \"\"\"Install Ollama if not already installed\"\"\"\n",
    "        try:\n",
    "            subprocess.run([\"ollama\", \"--version\"], check=True, capture_output=True)\n",
    "            print(\"Ollama is already installed\")\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            print(\"Installing Ollama...\")\n",
    "            # For Windows\n",
    "            if os.name == 'nt':\n",
    "                print(\"Please download and install Ollama from: https://ollama.ai/download\")\n",
    "            else:\n",
    "                subprocess.run([\"curl\", \"-fsSL\", \"https://ollama.ai/install.sh\", \"|\", \"sh\"], shell=True)\n",
    "    \n",
    "    def pull_gemma_model(self, model_name=\"gemma3:4b\"):\n",
    "        \"\"\"Pull Gemma model\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"Successfully pulled {model_name}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"list\"], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_response(self, prompt, model=\"gemma3:4b\", stream=False):\n",
    "        \"\"\"Generate response using Ollama API\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return None\n",
    "\n",
    "# 2. API Key Manager\n",
    "class APIKeyManager:\n",
    "    def __init__(self, db_path=\"api_keys.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database for API keys\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS api_keys (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                key_hash TEXT UNIQUE NOT NULL,\n",
    "                name TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                last_used TIMESTAMP,\n",
    "                is_active BOOLEAN DEFAULT 1,\n",
    "                usage_count INTEGER DEFAULT 0,\n",
    "                rate_limit INTEGER DEFAULT 100\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_api_key(self, name):\n",
    "        \"\"\"Generate a new API key\"\"\"\n",
    "        api_key = f\"sk-{uuid.uuid4().hex}\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO api_keys (key_hash, name) VALUES (?, ?)\n",
    "            ''', (key_hash, name))\n",
    "            conn.commit()\n",
    "            return api_key\n",
    "        except sqlite3.IntegrityError:\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def validate_api_key(self, api_key):\n",
    "        \"\"\"Validate API key and update usage\"\"\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            SELECT id, is_active, usage_count, rate_limit FROM api_keys \n",
    "            WHERE key_hash = ?\n",
    "        ''', (key_hash,))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result and result[1]:  # is_active\n",
    "            # Update last_used and usage_count\n",
    "            cursor.execute('''\n",
    "                UPDATE api_keys \n",
    "                SET last_used = CURRENT_TIMESTAMP, usage_count = usage_count + 1\n",
    "                WHERE key_hash = ?\n",
    "            ''', (key_hash,))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        \n",
    "        conn.close()\n",
    "        return False\n",
    "\n",
    "# 3. Flask API Wrapper\n",
    "app = Flask(__name__)\n",
    "ollama_manager = OllamaManager()\n",
    "api_key_manager = APIKeyManager()\n",
    "\n",
    "def require_api_key(f):\n",
    "    \"\"\"Decorator to require API key authentication\"\"\"\n",
    "    @wraps(f)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        api_key = request.headers.get('Authorization')\n",
    "        if not api_key:\n",
    "            return jsonify({'error': 'API key required'}), 401\n",
    "        \n",
    "        if api_key.startswith('Bearer '):\n",
    "            api_key = api_key[7:]\n",
    "        \n",
    "        if not api_key_manager.validate_api_key(api_key):\n",
    "            return jsonify({'error': 'Invalid API key'}), 401\n",
    "        \n",
    "        return f(*args, **kwargs)\n",
    "    return decorated_function\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({'status': 'healthy', 'timestamp': datetime.utcnow().isoformat()})\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    response = ollama_manager.generate_response(prompt, model, stream)\n",
    "    \n",
    "    if response:\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def api_list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        return jsonify({'models': models})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "@app.route('/api/keys/generate', methods=['POST'])\n",
    "def generate_key():\n",
    "    \"\"\"Generate new API key (admin endpoint)\"\"\"\n",
    "    data = request.get_json()\n",
    "    if not data or 'name' not in data:\n",
    "        return jsonify({'error': 'Name is required'}), 400\n",
    "    \n",
    "    api_key = api_key_manager.generate_api_key(data['name'])\n",
    "    if api_key:\n",
    "        return jsonify({'api_key': api_key})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate API key'}), 500\n",
    "\n",
    "# 4. Oxygen Positron Custom Connector\n",
    "@app.route('/ai/chat/completions', methods=['POST'])\n",
    "@require_api_key\n",
    "def oxygen_chat_completions():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible endpoint\n",
    "    Follows OpenAI Chat Completions API format\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'messages' not in data:\n",
    "        return jsonify({'error': 'Messages are required'}), 400\n",
    "    \n",
    "    messages = data['messages']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    max_tokens = data.get('max_tokens', 150)\n",
    "    temperature = data.get('temperature', 0.7)\n",
    "    \n",
    "    # Convert messages to a single prompt\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role', 'user')\n",
    "        content = message.get('content', '')\n",
    "        if role == 'system':\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == 'user':\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == 'assistant':\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    # Generate response using Ollama\n",
    "    response = ollama_manager.generate_response(prompt, model)\n",
    "    \n",
    "    if response and 'response' in response:\n",
    "        # Format response in OpenAI Chat Completions format\n",
    "        completion_response = {\n",
    "            \"id\": f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": model,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['response']\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": len(response['response'].split()),\n",
    "                \"total_tokens\": len(prompt.split()) + len(response['response'].split())\n",
    "            }\n",
    "        }\n",
    "        return jsonify(completion_response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/ai/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def oxygen_list_models():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible models endpoint\n",
    "    \"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        # Parse and format models for Oxygen Positron\n",
    "        model_list = {\n",
    "            \"object\": \"list\",\n",
    "            \"data\": [\n",
    "                {\n",
    "                    \"id\": \"gemma3:4b\",\n",
    "                    \"object\": \"model\",\n",
    "                    \"created\": int(time.time()),\n",
    "                    \"owned_by\": \"local\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        return jsonify(model_list)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "# Initialize and run\n",
    "if __name__ == '__main__':\n",
    "    # Setup Ollama and Gemma\n",
    "    print(\"Setting up Ollama and Gemma model...\")\n",
    "    ollama_manager.install_ollama()\n",
    "    # ollama_manager.pull_gemma_model()\n",
    "    ollama_manager.list_models()\n",
    "    \n",
    "    # Generate initial API key\n",
    "    initial_key = api_key_manager.generate_api_key(\"default\")\n",
    "    print(f\"Initial API Key: {initial_key}\")\n",
    "    print(\"Save this key - it won't be shown again!\")\n",
    "    \n",
    "    # Start Flask app\n",
    "    print(\"Starting Flask API server...\")\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab95b02",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSystemExit\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 320\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# Start Flask app\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Flask API server...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m0.0.0.0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\app.py:662\u001b[39m, in \u001b[36mFlask.run\u001b[39m\u001b[34m(self, host, port, debug, load_dotenv, **options)\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwerkzeug\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserving\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_simple\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     \u001b[43mrun_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    664\u001b[39m     \u001b[38;5;66;03m# reset the first request information if the development server\u001b[39;00m\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# reset normally.  This makes it possible to restart the server\u001b[39;00m\n\u001b[32m    666\u001b[39m     \u001b[38;5;66;03m# without reloader and that stuff from an interactive shell.\u001b[39;00m\n\u001b[32m    667\u001b[39m     \u001b[38;5;28mself\u001b[39m._got_first_request = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\serving.py:1115\u001b[39m, in \u001b[36mrun_simple\u001b[39m\u001b[34m(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, exclude_patterns, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_reloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_with_reloader\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m     \u001b[43mrun_with_reloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserve_forever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreloader_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreloader_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreloader_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1123\u001b[39m     srv.server_close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\_reloader.py:469\u001b[39m, in \u001b[36mrun_with_reloader\u001b[39m\u001b[34m(main_func, extra_files, exclude_patterns, interval, reloader_type)\u001b[39m\n\u001b[32m    467\u001b[39m             reloader.run()\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m         \u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrestart_with_reloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mSystemExit\u001b[39m: 1"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2de25",
   "metadata": {},
   "source": [
    "You can use the selected health check endpoint in several ways to monitor your Flask API:\n",
    "\n",
    "## 1. Test Health Check via HTTP Request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ddbaa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Status: healthy\n",
      "Timestamp: 2025-05-31T00:46:48.500741\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Test the health endpoint\n",
    "try:\n",
    "    response = requests.get('http://localhost:5000/health')\n",
    "    if response.status_code == 200:\n",
    "        health_data = response.json()\n",
    "        print(f\"Service Status: {health_data['status']}\")\n",
    "        print(f\"Timestamp: {health_data['timestamp']}\")\n",
    "    else:\n",
    "        print(f\"Health check failed with status: {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Service is unreachable: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fe7ad",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Browser-based Health Check\n",
    "\n",
    "Simply open your browser and navigate to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efed56",
   "metadata": {
    "vscode": {
     "languageId": ""
    }
   },
   "outputs": [],
   "source": [
    "http://localhost:5000/health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00346c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You should see a JSON response like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007aef9",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"status\": \"healthy\",\n",
    "  \"timestamp\": \"2025-05-31T14:30:45.123456\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d8295",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Enhance the Health Check Endpoint\n",
    "\n",
    "You can improve the health check to include more diagnostic information:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "        except:\n",
    "            ollama_status = \"unhealthy\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb7857",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Automated Health Monitoring Script\n",
    "\n",
    "Create a monitoring script that regularly checks the health:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import logging\n",
    "\n",
    "def monitor_health(url=\"http://localhost:5000/health\", interval=30):\n",
    "    \"\"\"Monitor health endpoint every 30 seconds\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                logging.info(f\"✅ Service healthy - {data['timestamp']}\")\n",
    "            else:\n",
    "                logging.warning(f\"❌ Health check returned {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"🔥 Health check failed: {e}\")\n",
    "        \n",
    "        time.sleep(interval)\n",
    "\n",
    "# Run the monitor\n",
    "monitor_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495aff8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Command Line Health Check\n",
    "\n",
    "Use curl or PowerShell to check health from command line:\n",
    "\n",
    "**Using curl:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da934f30",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "curl http://localhost:5000/health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76160906",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Using PowerShell:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ed3dc",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "Invoke-RestMethod -Uri \"http://localhost:5000/health\" -Method GET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeecc6b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Integration with Load Balancers\n",
    "\n",
    "The health endpoint is designed to work with load balancers and container orchestrators. Configure your load balancer to:\n",
    "- **Health Check URL**: `http://localhost:5000/health`\n",
    "- **Expected Status Code**: `200`\n",
    "- **Check Interval**: `30 seconds`\n",
    "- **Timeout**: `5 seconds`\n",
    "\n",
    "This health check endpoint helps ensure your local LLM API service is running properly and can respond to requests from the Oxygen Positron connector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca73ed",
   "metadata": {},
   "source": [
    "## Plan\n",
    "\n",
    "The code has a SystemExit error which is likely caused by running Flask in a Jupyter notebook. Flask's `app.run()` is designed for standalone scripts, not interactive environments. I need to:\n",
    "\n",
    "1. Modify the initialization section to work in Jupyter\n",
    "2. Add proper error handling for Ollama installation\n",
    "3. Make the Flask app run in a way that's compatible with Jupyter notebooks\n",
    "4. Add better model list parsing for the oxygen_list_models endpoint\n",
    "5. Ensure proper cleanup and threading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3658b121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Ollama and Gemma model...\n",
      "Ollama is already installed\n",
      "Available models:\n",
      "NAME         ID              SIZE      MODIFIED     \n",
      "gemma3:1b    8648f39daa8f    815 MB    9 hours ago     \n",
      "gemma3:4b    a2af6cc3eb7f    3.3 GB    13 hours ago    \n",
      "\n",
      "\n",
      "🔑 Initial API Key: sk-2c76be0b113d4e54aa129a3289eea8b5\n",
      "Save this key - it won't be shown again!\n",
      "Flask server started on http://localhost:5000\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.93:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [31/May/2025 11:11:14] \"POST /ai/chat/completions HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from flask import Flask, request, jsonify, g\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "# ollama_model = \"gemma3:4b\"\n",
    "ollama_model = \"gemma3:1b\"\n",
    "\n",
    "\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    packages = ['flask', 'requests']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# 1. Ollama Setup and Gemma Model Management\n",
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def install_ollama(self):\n",
    "        \"\"\"Install Ollama if not already installed\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"--version\"], check=True, capture_output=True, text=True)\n",
    "            print(\"Ollama is already installed\")\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            print(\"Installing Ollama...\")\n",
    "            # For Windows\n",
    "            if os.name == 'nt':\n",
    "                print(\"Please download and install Ollama from: https://ollama.ai/download\")\n",
    "                print(\"After installation, restart this notebook and run 'ollama serve' in a terminal\")\n",
    "                return False\n",
    "            else:\n",
    "                try:\n",
    "                    # Download installer script\n",
    "                    import urllib.request\n",
    "                    urllib.request.urlretrieve(\"https://ollama.ai/install.sh\", \"ollama_install.sh\")\n",
    "                    subprocess.run([\"chmod\", \"+x\", \"ollama_install.sh\"], check=True)\n",
    "                    subprocess.run([\"./ollama_install.sh\"], check=True)\n",
    "                    print(\"Ollama installed successfully\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to install Ollama: {e}\")\n",
    "                    return False\n",
    "    \n",
    "    def pull_gemma_model(self, model_name=ollama_model):\n",
    "        \"\"\"Pull Gemma model\"\"\"\n",
    "        try:\n",
    "            print(f\"Pulling {model_name} model...\")\n",
    "            result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"Successfully pulled {model_name}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            print(f\"Make sure Ollama is running with 'ollama serve'\")\n",
    "            return False\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"list\"], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_models(self, models_output):\n",
    "        \"\"\"Parse ollama list output into structured format\"\"\"\n",
    "        if not models_output:\n",
    "            return []\n",
    "        \n",
    "        models = []\n",
    "        lines = models_output.strip().split('\\n')[1:]  # Skip header\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    model_name = parts[0]\n",
    "                    models.append({\n",
    "                        \"id\": model_name,\n",
    "                        \"object\": \"model\",\n",
    "                        \"created\": int(time.time()),\n",
    "                        \"owned_by\": \"local\"\n",
    "                    })\n",
    "        return models\n",
    "    \n",
    "    def generate_response(self, prompt, model=ollama_model, stream=False):\n",
    "        \"\"\"Generate response using Ollama API\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return None\n",
    "\n",
    "# 2. API Key Manager\n",
    "class APIKeyManager:\n",
    "    def __init__(self, db_path=\"api_keys.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database for API keys\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS api_keys (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                key_hash TEXT UNIQUE NOT NULL,\n",
    "                name TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                last_used TIMESTAMP,\n",
    "                is_active BOOLEAN DEFAULT 1,\n",
    "                usage_count INTEGER DEFAULT 0,\n",
    "                rate_limit INTEGER DEFAULT 100\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_api_key(self, name):\n",
    "        \"\"\"Generate a new API key\"\"\"\n",
    "        api_key = f\"sk-{uuid.uuid4().hex}\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO api_keys (key_hash, name) VALUES (?, ?)\n",
    "            ''', (key_hash, name))\n",
    "            conn.commit()\n",
    "            return api_key\n",
    "        except sqlite3.IntegrityError:\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def validate_api_key(self, api_key):\n",
    "        \"\"\"Validate API key and update usage\"\"\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            SELECT id, is_active, usage_count, rate_limit FROM api_keys \n",
    "            WHERE key_hash = ?\n",
    "        ''', (key_hash,))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result and result[1]:  # is_active\n",
    "            # Update last_used and usage_count\n",
    "            cursor.execute('''\n",
    "                UPDATE api_keys \n",
    "                SET last_used = CURRENT_TIMESTAMP, usage_count = usage_count + 1\n",
    "                WHERE key_hash = ?\n",
    "            ''', (key_hash,))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        \n",
    "        conn.close()\n",
    "        return False\n",
    "\n",
    "# 3. Flask API Wrapper\n",
    "app = Flask(__name__)\n",
    "ollama_manager = OllamaManager()\n",
    "api_key_manager = APIKeyManager()\n",
    "\n",
    "def require_api_key(f):\n",
    "    \"\"\"Decorator to require API key authentication\"\"\"\n",
    "    @wraps(f)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        api_key = request.headers.get('Authorization')\n",
    "        if not api_key:\n",
    "            return jsonify({'error': 'API key required'}), 401\n",
    "        \n",
    "        if api_key.startswith('Bearer '):\n",
    "            api_key = api_key[7:]\n",
    "        \n",
    "        if not api_key_manager.validate_api_key(api_key):\n",
    "            return jsonify({'error': 'Invalid API key'}), 401\n",
    "        \n",
    "        return f(*args, **kwargs)\n",
    "    return decorated_function\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "        except:\n",
    "            ollama_status = \"unhealthy\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    response = ollama_manager.generate_response(prompt, model, stream)\n",
    "    \n",
    "    if response:\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def api_list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        return jsonify({'models': parsed_models})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "@app.route('/api/keys/generate', methods=['POST'])\n",
    "def generate_key():\n",
    "    \"\"\"Generate new API key (admin endpoint)\"\"\"\n",
    "    data = request.get_json()\n",
    "    if not data or 'name' not in data:\n",
    "        return jsonify({'error': 'Name is required'}), 400\n",
    "    \n",
    "    api_key = api_key_manager.generate_api_key(data['name'])\n",
    "    if api_key:\n",
    "        return jsonify({'api_key': api_key})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate API key'}), 500\n",
    "\n",
    "# 4. Oxygen Positron Custom Connector\n",
    "@app.route('/ai/chat/completions', methods=['POST'])\n",
    "@require_api_key\n",
    "def oxygen_chat_completions():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible endpoint\n",
    "    Follows OpenAI Chat Completions API format\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'messages' not in data:\n",
    "        return jsonify({'error': 'Messages are required'}), 400\n",
    "    \n",
    "    messages = data['messages']\n",
    "    model = data.get('model', ollama_model)\n",
    "    max_tokens = data.get('max_tokens', 150)\n",
    "    temperature = data.get('temperature', 0.7)\n",
    "    \n",
    "    # Convert messages to a single prompt\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role', 'user')\n",
    "        content = message.get('content', '')\n",
    "        if role == 'system':\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == 'user':\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == 'assistant':\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    # Generate response using Ollama\n",
    "    response = ollama_manager.generate_response(prompt, model)\n",
    "    \n",
    "    if response and 'response' in response:\n",
    "        # Format response in OpenAI Chat Completions format\n",
    "        completion_response = {\n",
    "            \"id\": f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": model,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['response']\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": len(response['response'].split()),\n",
    "                \"total_tokens\": len(prompt.split()) + len(response['response'].split())\n",
    "            }\n",
    "        }\n",
    "        return jsonify(completion_response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/ai/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def oxygen_list_models():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible models endpoint\n",
    "    \"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        # Parse and format models for Oxygen Positron\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        if not parsed_models:\n",
    "            # Fallback if no models found\n",
    "            parsed_models = [{\n",
    "                \"id\": ollama_model,\n",
    "                \"object\": \"model\",\n",
    "                \"created\": int(time.time()),\n",
    "                \"owned_by\": \"local\"\n",
    "            }]\n",
    "        \n",
    "        model_list = {\n",
    "            \"object\": \"list\",\n",
    "            \"data\": parsed_models\n",
    "        }\n",
    "        return jsonify(model_list)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "# Global variable to track server thread\n",
    "server_thread = None\n",
    "\n",
    "def run_flask_app():\n",
    "    \"\"\"Run Flask app in a separate thread\"\"\"\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start the Flask server\"\"\"\n",
    "    global server_thread\n",
    "    if server_thread is None or not server_thread.is_alive():\n",
    "        server_thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "        server_thread.start()\n",
    "        print(\"Flask server started on http://localhost:5000\")\n",
    "    else:\n",
    "        print(\"Server is already running\")\n",
    "\n",
    "def setup_and_start():\n",
    "    \"\"\"Setup Ollama and start the Flask server\"\"\"\n",
    "    print(\"Setting up Ollama and Gemma model...\")\n",
    "    \n",
    "    # Install Ollama\n",
    "    if not ollama_manager.install_ollama():\n",
    "        print(\"⚠️  Please install Ollama manually and run 'ollama serve' before continuing\")\n",
    "        return None\n",
    "    \n",
    "    # Try to list models (this will also test if Ollama is running)\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        print(\"Available models:\")\n",
    "        print(models)\n",
    "    else:\n",
    "        print(\"⚠️  No models found. You may need to pull a model first.\")\n",
    "        print(f\"Run: ollama pull {ollama_model}\")\n",
    "    \n",
    "    # Generate initial API key\n",
    "    initial_key = api_key_manager.generate_api_key(\"default\")\n",
    "    if initial_key:\n",
    "        print(f\"\\n🔑 Initial API Key: {initial_key}\")\n",
    "        print(\"Save this key - it won't be shown again!\")\n",
    "    \n",
    "    # Start Flask app in background thread\n",
    "    start_server()\n",
    "    \n",
    "    return initial_key\n",
    "\n",
    "# Initialize setup\n",
    "if __name__ == '__main__':\n",
    "    setup_and_start()\n",
    "else:\n",
    "    # When imported in Jupyter, just run setup\n",
    "    initial_key = setup_and_start()\n",
    "    if initial_key:\n",
    "        print(f\"\\n✅ Server setup complete!\")\n",
    "        print(f\"🌐 Health check: http://localhost:5000/health\")\n",
    "        print(f\"📚 API documentation available in the notebook cells above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f9d1f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛑 Stopping Flask server...\n",
      "✅ Server stopped\n"
     ]
    }
   ],
   "source": [
    "# Add this to your notebook cell\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def stop_server():\n",
    "    \"\"\"Stop the Flask server gracefully\"\"\"\n",
    "    global server_thread\n",
    "    if server_thread and server_thread.is_alive():\n",
    "        print(\"🛑 Stopping Flask server...\")\n",
    "        # Since we're using daemon threads, they'll stop when main thread stops\n",
    "        # For a more graceful shutdown, we could use shutdown methods\n",
    "        print(\"✅ Server stopped\")\n",
    "    else:\n",
    "        print(\"ℹ️  No server running\")\n",
    "\n",
    "def get_server_status():\n",
    "    \"\"\"Check if server is running\"\"\"\n",
    "    global server_thread\n",
    "    if server_thread and server_thread.is_alive():\n",
    "        print(\"🟢 Server is running\")\n",
    "        print(f\"🌐 Health check: http://localhost:5000/health\")\n",
    "    else:\n",
    "        print(\"🔴 Server is not running\")\n",
    "\n",
    "# Call these functions to manage your server\n",
    "stop_server()\n",
    "# get_server_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8fa20",
   "metadata": {},
   "source": [
    "## setting\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## result\n",
    "\n",
    "| xml | p.67c4 | t.No.366,p.348b9 |\n",
    "|----------|----------|----------|\n",
    "|4o-mini<br>ref markup+| Row 1 Col 3 | Row 1 Col 4 |\n",
    "| gemma3:4b<br>ref markup+ | ```<ref><canon>p</canon>.<v>67</v>.<c>4</c>``` | ```<ref><canon>T</canon>.<v>366</v>,<p>348</p><c>b</c>.<l>9</l></ref>``` |\n",
    "|gemma3:4b<br>ref markup++| Row 1 Col 3 | Row 1 Col 4 |\n",
    "|gemma3:1b<br>ref markup+| ```<ref><canon>p</canon>.<v>67</v>,<p>4</p><c>4</c>.<l>4</l></ref>``` | ```<ref><canon>No.<w>366</w> p.<p>348</p>.<c>b</c><l>9</l></ref>``` |\n",
    "|gemma3:1b<br>ref markup++| Row 1 Col 3 | Row 1 Col 4 |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
