{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb4c89d",
   "metadata": {},
   "source": [
    "# Local LLM with Intel GPU v2\n",
    "\n",
    "1. run Gemma model with ollama\n",
    "2. create RESTful API Wrapper with flask\n",
    "* create API key manager for API Wrapper \n",
    "* create wrapper for Oxygen Positron add-on by following:\n",
    "    * https://www.oxygenxml.com/doc/ug-addons/topics/ai-positron-custom-connector.html\n",
    "    * https://www.oxygenxml.com/doc/ug-addons/topics/ai_positron_enterprise.html\n",
    "    * https://github.com/oxygenxml/oxygen-ai-positron-custom-connector-addon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647a7d4",
   "metadata": {},
   "source": [
    "I'll help you create a complete solution for running Gemma model with Ollama and creating a RESTful API wrapper with Flask, including an API key manager and Oxygen Positron connector wrapper.\n",
    "\n",
    "## Step-by-step Solution:\n",
    "\n",
    "1. **Set up Ollama with Gemma model** - Install dependencies and pull the Gemma model\n",
    "2. **Create Flask API wrapper** - Build a RESTful API that interfaces with Ollama\n",
    "3. **Implement API key manager** - Add authentication and API key validation\n",
    "4. **Create Oxygen Positron connector** - Follow the specifications to create a compatible wrapper\n",
    "\n",
    "### [localLlmWIntelGPUV2.ipynb](file:///c%3A/Project/OCR/localLlmWIntelGPUV2.ipynb)\n",
    "\n",
    "Add implementation for running Gemma model with Ollama and creating Flask API wrapper with API key management and Oxygen Positron connector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4f28f1",
   "metadata": {},
   "source": [
    "This implementation provides:\n",
    "\n",
    "1. **Ollama Integration**: Automatic installation and Gemma model management\n",
    "2. **Flask API Wrapper**: RESTful API with proper error handling\n",
    "3. **API Key Management**: SQLite-based key storage with usage tracking\n",
    "4. **Oxygen Positron Compatibility**: OpenAI-compatible endpoints that work with the Oxygen XML Author add-on\n",
    "5. **Security**: API key authentication for all endpoints\n",
    "6. **Monitoring**: Usage tracking and health checks\n",
    "\n",
    "The solution follows the Oxygen Positron custom connector specifications and provides a complete local LLM setup with Intel GPU support via Ollama.\n",
    "\n",
    "Similar code found with 1 license type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975cd749",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### 1. Setup and Installation\n",
    "- Run the notebook cell above to install dependencies and start the server\n",
    "- The initial API key will be displayed - save it securely\n",
    "- Ollama will be installed (if not present) and Gemma model will be pulled\n",
    "\n",
    "### 2. API Endpoints\n",
    "\n",
    "#### Standard Endpoints:\n",
    "- `GET /health` - Health check\n",
    "- `POST /api/generate` - Generate text with Ollama\n",
    "- `GET /api/models` - List available models\n",
    "- `POST /api/keys/generate` - Generate new API key\n",
    "\n",
    "#### Oxygen Positron Compatible Endpoints:\n",
    "- `POST /ai/chat/completions` - OpenAI-compatible chat completions\n",
    "- `GET /ai/models` - OpenAI-compatible models list\n",
    "\n",
    "### 3. Authentication\n",
    "All endpoints (except health and key generation) require API key in header:\n",
    "```\n",
    "Authorization: Bearer your-api-key-here\n",
    "```\n",
    "\n",
    "### 4. Oxygen Positron Configuration\n",
    "In Oxygen XML Author, configure the custom connector with:\n",
    "- **Base URL**: `http://localhost:5000/ai`\n",
    "- **API Key**: Your generated API key\n",
    "- **Model**: `gemma3:4b`\n",
    "\n",
    "### 5. Example Usage\n",
    "```python\n",
    "import requests\n",
    "\n",
    "headers = {'Authorization': 'Bearer your-api-key'}\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Explain quantum computing\"}\n",
    "    ],\n",
    "    \"model\": \"gemma3:4b\"\n",
    "}\n",
    "\n",
    "response = requests.post('http://localhost:5000/ai/chat/completions', \n",
    "                        json=data, headers=headers)\n",
    "print(response.json())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bd293e",
   "metadata": {},
   "source": [
    "# Version 1.0 prototype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302efe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Ollama and Gemma model...\n",
      "Ollama is already installed\n",
      "Initial API Key: sk-1e2baeca84d14e9e9b1ee980cc1db424\n",
      "Save this key - it won't be shown again!\n",
      "Starting Flask API server...\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.0.6:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Project\\OCR\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3678: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from flask import Flask, request, jsonify, g\n",
    "import requests\n",
    "import sqlite3\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    packages = ['flask', 'requests', 'sqlite3']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# 1. Ollama Setup and Gemma Model Management\n",
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def install_ollama(self):\n",
    "        \"\"\"Install Ollama if not already installed\"\"\"\n",
    "        try:\n",
    "            subprocess.run([\"ollama\", \"--version\"], check=True, capture_output=True)\n",
    "            print(\"Ollama is already installed\")\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            print(\"Installing Ollama...\")\n",
    "            # For Windows\n",
    "            if os.name == 'nt':\n",
    "                print(\"Please download and install Ollama from: https://ollama.ai/download\")\n",
    "            else:\n",
    "                subprocess.run([\"curl\", \"-fsSL\", \"https://ollama.ai/install.sh\", \"|\", \"sh\"], shell=True)\n",
    "    \n",
    "    def pull_gemma_model(self, model_name=\"gemma3:4b\"):\n",
    "        \"\"\"Pull Gemma model\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"Successfully pulled {model_name}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"list\"], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def generate_response(self, prompt, model=\"gemma3:4b\", stream=False):\n",
    "        \"\"\"Generate response using Ollama API\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return None\n",
    "\n",
    "# 2. API Key Manager\n",
    "class APIKeyManager:\n",
    "    def __init__(self, db_path=\"api_keys.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database for API keys\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS api_keys (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                key_hash TEXT UNIQUE NOT NULL,\n",
    "                name TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                last_used TIMESTAMP,\n",
    "                is_active BOOLEAN DEFAULT 1,\n",
    "                usage_count INTEGER DEFAULT 0,\n",
    "                rate_limit INTEGER DEFAULT 100\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_api_key(self, name):\n",
    "        \"\"\"Generate a new API key\"\"\"\n",
    "        api_key = f\"sk-{uuid.uuid4().hex}\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO api_keys (key_hash, name) VALUES (?, ?)\n",
    "            ''', (key_hash, name))\n",
    "            conn.commit()\n",
    "            return api_key\n",
    "        except sqlite3.IntegrityError:\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def validate_api_key(self, api_key):\n",
    "        \"\"\"Validate API key and update usage\"\"\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            SELECT id, is_active, usage_count, rate_limit FROM api_keys \n",
    "            WHERE key_hash = ?\n",
    "        ''', (key_hash,))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result and result[1]:  # is_active\n",
    "            # Update last_used and usage_count\n",
    "            cursor.execute('''\n",
    "                UPDATE api_keys \n",
    "                SET last_used = CURRENT_TIMESTAMP, usage_count = usage_count + 1\n",
    "                WHERE key_hash = ?\n",
    "            ''', (key_hash,))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        \n",
    "        conn.close()\n",
    "        return False\n",
    "\n",
    "# 3. Flask API Wrapper\n",
    "app = Flask(__name__)\n",
    "ollama_manager = OllamaManager()\n",
    "api_key_manager = APIKeyManager()\n",
    "\n",
    "def require_api_key(f):\n",
    "    \"\"\"Decorator to require API key authentication\"\"\"\n",
    "    @wraps(f)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        api_key = request.headers.get('Authorization')\n",
    "        if not api_key:\n",
    "            return jsonify({'error': 'API key required'}), 401\n",
    "        \n",
    "        if api_key.startswith('Bearer '):\n",
    "            api_key = api_key[7:]\n",
    "        \n",
    "        if not api_key_manager.validate_api_key(api_key):\n",
    "            return jsonify({'error': 'Invalid API key'}), 401\n",
    "        \n",
    "        return f(*args, **kwargs)\n",
    "    return decorated_function\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({'status': 'healthy', 'timestamp': datetime.utcnow().isoformat()})\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    response = ollama_manager.generate_response(prompt, model, stream)\n",
    "    \n",
    "    if response:\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def api_list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        return jsonify({'models': models})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "@app.route('/api/keys/generate', methods=['POST'])\n",
    "def generate_key():\n",
    "    \"\"\"Generate new API key (admin endpoint)\"\"\"\n",
    "    data = request.get_json()\n",
    "    if not data or 'name' not in data:\n",
    "        return jsonify({'error': 'Name is required'}), 400\n",
    "    \n",
    "    api_key = api_key_manager.generate_api_key(data['name'])\n",
    "    if api_key:\n",
    "        return jsonify({'api_key': api_key})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate API key'}), 500\n",
    "\n",
    "# 4. Oxygen Positron Custom Connector\n",
    "@app.route('/ai/chat/completions', methods=['POST'])\n",
    "@require_api_key\n",
    "def oxygen_chat_completions():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible endpoint\n",
    "    Follows OpenAI Chat Completions API format\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'messages' not in data:\n",
    "        return jsonify({'error': 'Messages are required'}), 400\n",
    "    \n",
    "    messages = data['messages']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    max_tokens = data.get('max_tokens', 150)\n",
    "    temperature = data.get('temperature', 0.7)\n",
    "    \n",
    "    # Convert messages to a single prompt\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role', 'user')\n",
    "        content = message.get('content', '')\n",
    "        if role == 'system':\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == 'user':\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == 'assistant':\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    # Generate response using Ollama\n",
    "    response = ollama_manager.generate_response(prompt, model)\n",
    "    \n",
    "    if response and 'response' in response:\n",
    "        # Format response in OpenAI Chat Completions format\n",
    "        completion_response = {\n",
    "            \"id\": f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": model,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['response']\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": len(response['response'].split()),\n",
    "                \"total_tokens\": len(prompt.split()) + len(response['response'].split())\n",
    "            }\n",
    "        }\n",
    "        return jsonify(completion_response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/ai/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def oxygen_list_models():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible models endpoint\n",
    "    \"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        # Parse and format models for Oxygen Positron\n",
    "        model_list = {\n",
    "            \"object\": \"list\",\n",
    "            \"data\": [\n",
    "                {\n",
    "                    \"id\": \"gemma3:4b\",\n",
    "                    \"object\": \"model\",\n",
    "                    \"created\": int(time.time()),\n",
    "                    \"owned_by\": \"local\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        return jsonify(model_list)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "# Initialize and run\n",
    "if __name__ == '__main__':\n",
    "    # Setup Ollama and Gemma\n",
    "    print(\"Setting up Ollama and Gemma model...\")\n",
    "    ollama_manager.install_ollama()\n",
    "    # ollama_manager.pull_gemma_model()\n",
    "    ollama_manager.list_models()\n",
    "    \n",
    "    # Generate initial API key\n",
    "    initial_key = api_key_manager.generate_api_key(\"default\")\n",
    "    print(f\"Initial API Key: {initial_key}\")\n",
    "    print(\"Save this key - it won't be shown again!\")\n",
    "    \n",
    "    # Start Flask app\n",
    "    print(\"Starting Flask API server...\")\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab95b02",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSystemExit\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 320\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# Start Flask app\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Flask API server...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m320\u001b[39m \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m0.0.0.0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\app.py:662\u001b[39m, in \u001b[36mFlask.run\u001b[39m\u001b[34m(self, host, port, debug, load_dotenv, **options)\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwerkzeug\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mserving\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_simple\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     \u001b[43mrun_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    664\u001b[39m     \u001b[38;5;66;03m# reset the first request information if the development server\u001b[39;00m\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# reset normally.  This makes it possible to restart the server\u001b[39;00m\n\u001b[32m    666\u001b[39m     \u001b[38;5;66;03m# without reloader and that stuff from an interactive shell.\u001b[39;00m\n\u001b[32m    667\u001b[39m     \u001b[38;5;28mself\u001b[39m._got_first_request = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\serving.py:1115\u001b[39m, in \u001b[36mrun_simple\u001b[39m\u001b[34m(hostname, port, application, use_reloader, use_debugger, use_evalex, extra_files, exclude_patterns, reloader_interval, reloader_type, threaded, processes, request_handler, static_files, passthrough_errors, ssl_context)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_reloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_with_reloader\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m     \u001b[43mrun_with_reloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mserve_forever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1118\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1119\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreloader_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreloader_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreloader_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1123\u001b[39m     srv.server_close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\werkzeug\\_reloader.py:469\u001b[39m, in \u001b[36mrun_with_reloader\u001b[39m\u001b[34m(main_func, extra_files, exclude_patterns, interval, reloader_type)\u001b[39m\n\u001b[32m    467\u001b[39m             reloader.run()\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m         \u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrestart_with_reloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mSystemExit\u001b[39m: 1"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c2de25",
   "metadata": {},
   "source": [
    "You can use the selected health check endpoint in several ways to monitor your Flask API:\n",
    "\n",
    "## 1. Test Health Check via HTTP Request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ddbaa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /health HTTP/1.1\" 200 143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Status: healthy\n",
      "Timestamp: 2025-06-03T16:28:15.946289\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Test the health endpoint\n",
    "try:\n",
    "    response = requests.get('http://localhost:5000/health')\n",
    "    if response.status_code == 200:\n",
    "        health_data = response.json()\n",
    "        print(f\"Service Status: {health_data['status']}\")\n",
    "        print(f\"Timestamp: {health_data['timestamp']}\")\n",
    "    else:\n",
    "        print(f\"Health check failed with status: {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Service is unreachable: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fe7ad",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Browser-based Health Check\n",
    "\n",
    "Simply open your browser and navigate to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:5000/health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00346c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "You should see a JSON response like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6007aef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"status\": \"healthy\",\n",
    "  \"timestamp\": \"2025-05-31T14:30:45.123456\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654d8295",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Enhance the Health Check Endpoint\n",
    "\n",
    "You can improve the health check to include more diagnostic information:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba4814",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "        except:\n",
    "            ollama_status = \"unhealthy\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb7857",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Automated Health Monitoring Script\n",
    "\n",
    "Create a monitoring script that regularly checks the health:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd9d64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /health HTTP/1.1\" 200 143\n",
      "INFO:root:✅ Service healthy - 2025-06-03T16:28:40.078615\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /health HTTP/1.1\" 200 143\n",
      "INFO:root:✅ Service healthy - 2025-06-03T16:29:12.201205\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import logging\n",
    "\n",
    "def monitor_health(url=\"http://localhost:5000/health\", interval=30):\n",
    "    \"\"\"Monitor health endpoint every 30 seconds\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                logging.info(f\"✅ Service healthy - {data['timestamp']}\")\n",
    "            else:\n",
    "                logging.warning(f\"❌ Health check returned {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"🔥 Health check failed: {e}\")\n",
    "        \n",
    "        time.sleep(interval)\n",
    "\n",
    "# Run the monitor\n",
    "monitor_health()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6495aff8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Command Line Health Check\n",
    "\n",
    "Use curl or PowerShell to check health from command line:\n",
    "\n",
    "**Using curl:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da934f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl http://localhost:5000/health"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76160906",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Using PowerShell:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ed3dc",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "Invoke-RestMethod -Uri \"http://localhost:5000/health\" -Method GET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeecc6b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Integration with Load Balancers\n",
    "\n",
    "The health endpoint is designed to work with load balancers and container orchestrators. Configure your load balancer to:\n",
    "- **Health Check URL**: `http://localhost:5000/health`\n",
    "- **Expected Status Code**: `200`\n",
    "- **Check Interval**: `30 seconds`\n",
    "- **Timeout**: `5 seconds`\n",
    "\n",
    "This health check endpoint helps ensure your local LLM API service is running properly and can respond to requests from the Oxygen Positron connector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca73ed",
   "metadata": {},
   "source": [
    "# Version 2.0 without GPU\n",
    "\n",
    "## Plan\n",
    "\n",
    "The code has a SystemExit error which is likely caused by running Flask in a Jupyter notebook. Flask's `app.run()` is designed for standalone scripts, not interactive environments. I need to:\n",
    "\n",
    "1. Modify the initialization section to work in Jupyter\n",
    "2. Add proper error handling for Ollama installation\n",
    "3. Make the Flask app run in a way that's compatible with Jupyter notebooks\n",
    "4. Add better model list parsing for the oxygen_list_models endpoint\n",
    "5. Ensure proper cleanup and threading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658b121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Ollama and Gemma model...\n",
      "Ollama is already installed\n",
      "Available models:\n",
      "NAME          ID              SIZE      MODIFIED     \n",
      "gemma3:4b     a2af6cc3eb7f    3.3 GB    8 hours ago     \n",
      "gemma3:1b     8648f39daa8f    815 MB    8 hours ago     \n",
      "gemma3:12b    f4031aab637d    8.1 GB    40 hours ago    \n",
      "\n",
      "Flask server started on http://localhost:5000\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.93:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from flask import Flask, request, jsonify, g\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "ollama_default_model = \"gemma3:1b\"\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    packages = ['flask', 'requests']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# 1. Ollama Setup and Gemma Model Management\n",
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def install_ollama(self):\n",
    "        \"\"\"Install Ollama if not already installed\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"--version\"], check=True, capture_output=True, text=True)\n",
    "            print(\"Ollama is already installed\")\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            print(\"Installing Ollama...\")\n",
    "            # For Windows\n",
    "            if os.name == 'nt':\n",
    "                print(\"Please download and install Ollama from: https://ollama.ai/download\")\n",
    "                print(\"After installation, restart this notebook and run 'ollama serve' in a terminal\")\n",
    "                return False\n",
    "            else:\n",
    "                try:\n",
    "                    # Download installer script\n",
    "                    import urllib.request\n",
    "                    urllib.request.urlretrieve(\"https://ollama.ai/install.sh\", \"ollama_install.sh\")\n",
    "                    subprocess.run([\"chmod\", \"+x\", \"ollama_install.sh\"], check=True)\n",
    "                    subprocess.run([\"./ollama_install.sh\"], check=True)\n",
    "                    print(\"Ollama installed successfully\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to install Ollama: {e}\")\n",
    "                    return False\n",
    "    \n",
    "    def pull_gemma_model(self, model_name=ollama_default_model):\n",
    "        \"\"\"Pull Gemma model\"\"\"\n",
    "        try:\n",
    "            print(f\"Pulling {model_name} model...\")\n",
    "            result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"Successfully pulled {model_name}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            print(f\"Make sure Ollama is running with 'ollama serve'\")\n",
    "            return False\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"list\"], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_models(self, models_output):\n",
    "        \"\"\"Parse ollama list output into structured format\"\"\"\n",
    "        if not models_output:\n",
    "            return []\n",
    "        \n",
    "        models = []\n",
    "        lines = models_output.strip().split('\\n')[1:]  # Skip header\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    model_name = parts[0]\n",
    "                    models.append({\n",
    "                        \"id\": model_name,\n",
    "                        \"object\": \"model\",\n",
    "                        \"created\": int(time.time()),\n",
    "                        \"owned_by\": \"local\"\n",
    "                    })\n",
    "        return models\n",
    "    \n",
    "    def generate_response(self, prompt, model=ollama_default_model, stream=False):\n",
    "        \"\"\"Generate response using Ollama API with extended timeout\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            \"options\": {\n",
    "                \"num_predict\": 3000,  # Limit tokens to speed up\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Use longer timeout for qwen models\n",
    "        if \"qwen\" in model.lower():\n",
    "            timeout = 480 \n",
    "        elif \"deepseek\" in model.lower() or \"llama\" in model.lower() or \"mistral\" in model.lower():\n",
    "            timeout = 180 \n",
    "        elif \"gemma3:12b\" in model.lower():\n",
    "            timeout = 360 \n",
    "        else:\n",
    "            timeout = 100\n",
    "        # timeout = 480 if \"qwen\" in model.lower() or \"deepseek\" in model.lower() else 60\n",
    "        \n",
    "        try:\n",
    "            print(f\"Generating with {model} (timeout: {timeout}s)...\")\n",
    "            response = requests.post(url, json=payload, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            # print(json.dumps(response.json(), indent=2))\n",
    "            result = response.json()\n",
    "            if 'response' in result:\n",
    "                print(result['response'])\n",
    "            else:\n",
    "                print(json.dumps(result, indent=2))\n",
    "            return response.json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout after {timeout} seconds with model {model}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return None\n",
    "\n",
    "# 2. API Key Manager\n",
    "class APIKeyManager:\n",
    "    def __init__(self, db_path=\"api_keys.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database for API keys\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS api_keys (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                key_hash TEXT UNIQUE NOT NULL,\n",
    "                name TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                last_used TIMESTAMP,\n",
    "                is_active BOOLEAN DEFAULT 1,\n",
    "                usage_count INTEGER DEFAULT 0,\n",
    "                rate_limit INTEGER DEFAULT 100\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_api_key(self, name):\n",
    "        \"\"\"Generate a new API key\"\"\"\n",
    "        api_key = f\"sk-{uuid.uuid4().hex}\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO api_keys (key_hash, name) VALUES (?, ?)\n",
    "            ''', (key_hash, name))\n",
    "            conn.commit()\n",
    "            return api_key\n",
    "        except sqlite3.IntegrityError:\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def validate_api_key(self, api_key):\n",
    "        \"\"\"Validate API key and update usage\"\"\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            SELECT id, is_active, usage_count, rate_limit FROM api_keys \n",
    "            WHERE key_hash = ?\n",
    "        ''', (key_hash,))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result and result[1]:  # is_active\n",
    "            # Update last_used and usage_count\n",
    "            cursor.execute('''\n",
    "                UPDATE api_keys \n",
    "                SET last_used = CURRENT_TIMESTAMP, usage_count = usage_count + 1\n",
    "                WHERE key_hash = ?\n",
    "            ''', (key_hash,))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        \n",
    "        conn.close()\n",
    "        return False\n",
    "\n",
    "# 3. Flask API Wrapper\n",
    "app = Flask(__name__)\n",
    "ollama_manager = OllamaManager()\n",
    "api_key_manager = APIKeyManager()\n",
    "\n",
    "def require_api_key(f):\n",
    "    \"\"\"Decorator to require API key authentication\"\"\"\n",
    "    @wraps(f)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        api_key = request.headers.get('Authorization')\n",
    "        if not api_key:\n",
    "            return jsonify({'error': 'API key required'}), 401\n",
    "        \n",
    "        if api_key.startswith('Bearer '):\n",
    "            api_key = api_key[7:]\n",
    "        \n",
    "        if not api_key_manager.validate_api_key(api_key):\n",
    "            return jsonify({'error': 'Invalid API key'}), 401\n",
    "        \n",
    "        return f(*args, **kwargs)\n",
    "    return decorated_function\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "        except:\n",
    "            ollama_status = \"unhealthy\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    response = ollama_manager.generate_response(prompt, model, stream)\n",
    "    \n",
    "    if response:\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def api_list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        return jsonify({'models': parsed_models})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "@app.route('/api/keys/generate', methods=['POST'])\n",
    "def generate_key():\n",
    "    \"\"\"Generate new API key (admin endpoint)\"\"\"\n",
    "    data = request.get_json()\n",
    "    if not data or 'name' not in data:\n",
    "        return jsonify({'error': 'Name is required'}), 400\n",
    "    \n",
    "    api_key = api_key_manager.generate_api_key(data['name'])\n",
    "    if api_key:\n",
    "        return jsonify({'api_key': api_key})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate API key'}), 500\n",
    "\n",
    "# 4. Oxygen Positron Custom Connector\n",
    "@app.route('/ai/chat/completions', methods=['POST'])\n",
    "@require_api_key\n",
    "def oxygen_chat_completions():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible endpoint\n",
    "    Follows OpenAI Chat Completions API format`\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    print(f\"Received data: {data}\")\n",
    "    \n",
    "    if not data or 'messages' not in data:\n",
    "        return jsonify({'error': 'Messages are required'}), 400\n",
    "    \n",
    "    messages = data['messages']\n",
    "    model = data.get('model', ollama_default_model)\n",
    "    max_tokens = data.get('max_tokens', 150)\n",
    "    temperature = data.get('temperature', 0.7)\n",
    "    \n",
    "    # Convert messages to a single prompt\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role', 'user')\n",
    "        content = message.get('content', '')\n",
    "        if role == 'system':\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == 'user':\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == 'assistant':\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    # Generate response using Ollama\n",
    "    response = ollama_manager.generate_response(prompt, model)\n",
    "    \n",
    "    if response and 'response' in response:\n",
    "        # Format response in OpenAI Chat Completions format\n",
    "        completion_response = {\n",
    "            \"id\": f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": model,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['response']\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": len(response['response'].split()),\n",
    "                \"total_tokens\": len(prompt.split()) + len(response['response'].split())\n",
    "            }\n",
    "        }\n",
    "        return jsonify(completion_response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/ai/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def oxygen_list_models():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible models endpoint\n",
    "    \"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        # Parse and format models for Oxygen Positron\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        if not parsed_models:\n",
    "            # Fallback if no models found\n",
    "            parsed_models = [{\n",
    "                \"id\": ollama_default_model,\n",
    "                \"object\": \"model\",\n",
    "                \"created\": int(time.time()),\n",
    "                \"owned_by\": \"local\"\n",
    "            }]\n",
    "        \n",
    "        model_list = {\n",
    "            \"object\": \"list\",\n",
    "            \"data\": parsed_models\n",
    "        }\n",
    "        return jsonify(model_list)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "# Global variable to track server thread\n",
    "server_thread = None\n",
    "\n",
    "def run_flask_app():\n",
    "    \"\"\"Run Flask app in a separate thread\"\"\"\n",
    "    # Enable Flask debug logging\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    app.logger.setLevel(logging.DEBUG)\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start the Flask server\"\"\"\n",
    "    global server_thread\n",
    "    if server_thread is None or not server_thread.is_alive():\n",
    "        server_thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "        server_thread.start()\n",
    "        print(\"Flask server started on http://localhost:5000\")\n",
    "    else:\n",
    "        print(\"Server is already running\")\n",
    "\n",
    "def setup_and_start():\n",
    "    \"\"\"Setup Ollama and start the Flask server\"\"\"\n",
    "    print(\"Setting up Ollama and Gemma model...\")\n",
    "    \n",
    "    # Install Ollama\n",
    "    if not ollama_manager.install_ollama():\n",
    "        print(\"⚠️  Please install Ollama manually and run 'ollama serve' before continuing\")\n",
    "        return None\n",
    "    \n",
    "    # Try to list models (this will also test if Ollama is running)\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        print(\"Available models:\")\n",
    "        print(models)\n",
    "    else:\n",
    "        print(\"⚠️  No models found. You may need to pull a model first.\")\n",
    "        print(f\"Run: ollama pull {ollama_default_model}\")\n",
    "    \n",
    "    # Generate initial API key\n",
    "    # initial_key = api_key_manager.generate_api_key(\"default\")\n",
    "    # if initial_key:\n",
    "    #     print(f\"\\n🔑 Initial API Key: {initial_key}\")\n",
    "    #     print(\"Save this key - it won't be shown again!\")\n",
    "    \n",
    "    # Start Flask app in background thread\n",
    "    start_server()\n",
    "    \n",
    "    # return initial_key\n",
    "\n",
    "# Initialize setup\n",
    "if __name__ == '__main__':\n",
    "    setup_and_start()\n",
    "else:\n",
    "    # When imported in Jupyter, just run setup\n",
    "    initial_key = setup_and_start()\n",
    "    if initial_key:\n",
    "        print(f\"\\n✅ Server setup complete!\")\n",
    "        print(f\"🌐 Health check: http://localhost:5000/health\")\n",
    "        print(f\"📚 API documentation available in the notebook cells above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6af86eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models output:\n",
      "NAME          ID              SIZE      MODIFIED     \n",
      "gemma3:4b     a2af6cc3eb7f    3.3 GB    7 hours ago     \n",
      "gemma3:1b     8648f39daa8f    815 MB    7 hours ago     \n",
      "gemma3:12b    f4031aab637d    8.1 GB    39 hours ago    \n",
      "\n",
      "Parsed models:\n",
      "- gemma3:4b\n",
      "- gemma3:1b\n",
      "- gemma3:12b\n"
     ]
    }
   ],
   "source": [
    "models_output = ollama_manager.list_models()\n",
    "print(\"Models output:\")\n",
    "print(models_output)\n",
    "\n",
    "# Parse models from the output\n",
    "parsed_models = ollama_manager.parse_models(models_output)\n",
    "print(\"Parsed models:\")\n",
    "for model in parsed_models:\n",
    "\tprint(f\"- {model['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c641a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:5000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:5000 \"GET /api/models HTTP/1.1\" 200 260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"models\": [\n",
      "    {\n",
      "      \"created\": 1748968236,\n",
      "      \"id\": \"llama3.1:8b\",\n",
      "      \"object\": \"model\",\n",
      "      \"owned_by\": \"local\"\n",
      "    },\n",
      "    {\n",
      "      \"created\": 1748968236,\n",
      "      \"id\": \"gemma3:12b\",\n",
      "      \"object\": \"model\",\n",
      "      \"owned_by\": \"local\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with your actual API key\n",
    "api_key = \"sk-2c76be0b113d4e54aa129a3289eea8b5\"\n",
    "headers = {'Authorization': f'Bearer {api_key}'}\n",
    "\n",
    "response = requests.get('http://localhost:5000/api/models', headers=headers)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed8fa20",
   "metadata": {},
   "source": [
    "## setting\n",
    "\n",
    "1. Plugins / Oxygen AI Positron Assistant / AI Service Configuration\n",
    "* Address: `https://aipositron.oxygenxml.com`\n",
    "* Model: `by selection from GPT-4o, GPT-4o mini....`\n",
    "\n",
    "2. Plugins / Oxygen AI Positron Assistant `Enterprise` / AI Service Configuration\n",
    "* AI connector: `Custom AI service`\n",
    "* Address: `http://127.0.0.1:5000/ai/`\n",
    "* API key: `from flask API key manager`\n",
    "* Model: `by selection from gemma3:1b, gemma3:4b, deepseek-r1:1.5b(T), qwen3:4b(T)`\n",
    "* Enable text moderation: `unchecked`\n",
    "* Enable streaming: `checked`\n",
    "\n",
    "\n",
    "## result\n",
    "\n",
    "| AI Positron | model | Action\\xml | p.67c4 | t.No.366,p.348b9 |\n",
    "|----------|----------|----------|----------|----------|\n",
    "| AI Positron |GPT-4o|ref markup opt| ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |GPT-4o mini(8b) | | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "| Enterprise<br>+CustomAI |gemma3:12b<br>⚠️slower| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:4b| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<v>366</v>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |llama3.1:8b<br>⚠️slow| | ⚠️❌```<p.<p>67</p><c>c</c><l>4</l></ref>``` | ⚠️✅```<ref><canon>T</canon>.No.<w>366</w>, p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "| AI Positron |GPT-4o|ref markup++| ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |GPT-4o mini(8b) | | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "| Enterprise<br>+CustomAI |gemma3:12b<br>⚠️slower| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>no.366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:4b| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>no.366</w> p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:1b| | ❌```<ref><canon>p.<67</p><c>4</c>``` | ❌```<ref><canon>T</canon>.<v>366</v>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |llama3.1:8b<br>⚠️slow| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ✅```<ref><canon>T</canon>.No.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |mistral:7b<br>⚠️slow| | ❌``` <ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌``` <ref><canon>T</canon>.<v>No.</v>366,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |qwen3:8b(T)<br>⚠️very slow| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>366</w> p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |qwen3:4b(T)<br>⚠️very slow| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon> No.<w>366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |deepseek-r1:1.5b(T)<br>⚠️unstable| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>t.No.366,p.348b9</canon></ref><v>366</v><p>.348</p><c>b</c><l>9</l>``` |\n",
    "| AI Positron |GPT-4o|ref markup+| ❌```<ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.No.<w>366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |GPT-4o mini(8b)| | ❌```<ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.No.<w>366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "| Enterprise<br>+CustomAI |gemma3:12b<br>⚠️slower| | ✅```<ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>no.366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:4b| | ❌```<ref><canon>T</canon>.<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>366</w>,p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |gemma3:1b| | ❌```<ref><canon>p</canon>.<p>67</p><c>4</c>``` | ❌```<ref><canon>T</canon>.<v>366</v>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |llama3.1:8b<br>⚠️slow| | ⚠️✅``` <ref>p.<p>67</p><c>c</c><l>4</l></ref>``` | ⚠️✅```<ref><canon>T</canon>.No.<w>366</w>, p.<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |mistral:7b<br>⚠️slow| | ❌``` <ref><canon>T</canon>.<v>...</v>,<p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<v>unknown</v>,<p>67</p><c>c</c><l>4</l></ref>``` |\n",
    "|  |qwen3:8b(T)<br>⚠️very slow| | ❌```<ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>No.366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |qwen3:4b(T)<br>⚠️very slow| | ❌```<ref><p>67</p><c>c</c><l>4</l></ref>``` | ❌```<ref><canon>T</canon>.<w>366</w>,<p>348</p><c>b</c><l>9</l></ref>``` |\n",
    "|  |deepseek-r1:1.5b(T)<br>⚠️unstable| | ❌```<ref><canon>p.67</canon><v>67</v><p>67</p><c>4</c><l>1</l>-<l>296</l></ref>``` | ❌```<ref><canon>T</canon>.<v>366</v>,<p>348</p><c>b</c><l>0</l>-<l>1</l></ref>``` |\n",
    "\n",
    "* extra comment from llama3.1:8b w ref markup+:\n",
    "    * ```It seems like you are entering a single page reference, I will just markup it with <p>, <c> and <l> elements.```\n",
    "    * ```Would you like to add the <ref> element around it as well?```\n",
    "\n",
    "* llama3.1:8b\n",
    "    * ```<p.67<c>c</c><l>4</l></ref>```\n",
    "    * ```<p.<<p>>67</p><c>c</c><l>4</l></ref>```\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243d5a1",
   "metadata": {},
   "source": [
    "❌ gemma3:12b failed with status: 500\n",
    "\n",
    "* Error: {\"error\":\"model requires more system memory (11.3 GiB) than is available (8.4 GiB)\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15674f1f",
   "metadata": {},
   "source": [
    "## pull and rm models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f6f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling deepseek-r1:1.5b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-25 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled deepseek-r1:1.5b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "899faca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling qwen3:4b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled qwen3:4b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"qwen3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d08706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling gemma3:1b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-37 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled gemma3:1b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"gemma3:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be6020c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling gemma3:4b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-39 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled gemma3:4b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"gemma3:4b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cddfeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling gemma3:12b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled gemma3:12b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"gemma3:12b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5759b760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling qwen3:8b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-101 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled qwen3:8b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"qwen3:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a9da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling llama3.1:8b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-17 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled llama3.1:8b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"llama3.1:8b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29328c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling mistral:7b model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-14 (_readerthread):\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Project\\OCR\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\jeffw\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\subprocess.py\", line 1599, in _readerthread\n",
      "    buffer.append(fh.read())\n",
      "                  ^^^^^^^^^\n",
      "UnicodeDecodeError: 'cp950' codec can't decode byte 0xe2 in position 35: illegal multibyte sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pulled mistral:7b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_manager.pull_gemma_model(\"mistral:7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14515933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing model: llama3.1:8b\n",
      "✅ Model llama3.1:8b removed successfully!\n",
      "deleted 'llama3.1:8b'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def remove_ollama_model(model_name):\n",
    "    \"\"\"Remove a specific model from Ollama\"\"\"\n",
    "    try:\n",
    "        print(f\"Removing model: {model_name}\")\n",
    "        result = subprocess.run(['ollama', 'rm', model_name], \n",
    "                              capture_output=True, text=True, shell=True)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ Model {model_name} removed successfully!\")\n",
    "            print(result.stdout)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Failed to remove model {model_name}\")\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error removing model: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# # Example: Remove gemma3:1b model\n",
    "# remove_ollama_model('gemma3:12b')\n",
    "# remove_ollama_model('qwen3:4b')\n",
    "# remove_ollama_model('gemma3:4b')\n",
    "# remove_ollama_model('deepseek-r1:1.5b')\n",
    "# remove_ollama_model('qwen3:8b')\n",
    "# remove_ollama_model('mistral:7b')\n",
    "remove_ollama_model('llama3.1:8b')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25c5091d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME          ID              SIZE      MODIFIED               \n",
      "gemma3:4b     a2af6cc3eb7f    3.3 GB    Less than a second ago    \n",
      "gemma3:1b     8648f39daa8f    815 MB    3 minutes ago             \n",
      "gemma3:12b    f4031aab637d    8.1 GB    32 hours ago              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = ollama_manager.list_models()\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83523002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME           ID              SIZE      MODIFIED      \n",
      "llama3.1:8b    46e0c10c039e    4.9 GB    7 minutes ago    \n",
      "gemma3:12b     f4031aab637d    8.1 GB    3 hours ago      \n",
      "\n",
      "Generating with llama3.1:8b (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 913\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a computer program, so I don't have feelings in the same way humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How about you? Is there something on your mind that you'd like to chat about or ask for assistance with?\n",
      "I'm just a computer program, so I don't have feelings in the same way humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How about you? Is there something on your mind that you'd like to chat about or ask for assistance with?\n",
      "Generating with llama3.1:8b (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 423\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n",
      "The capital of France is Paris.\n",
      "Generating with gemma3:12b (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 826\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm doing well, thank you for asking! As an AI, I don't experience feelings like humans do, but my systems are running smoothly and I'm ready to chat. 😊 \n",
      "\n",
      "How are *you* doing today?\n",
      "Hello! I'm doing well, thank you for asking! As an AI, I don't experience feelings like humans do, but my systems are running smoothly and I'm ready to chat. 😊 \n",
      "\n",
      "How are *you* doing today?\n",
      "Generating with gemma3:12b (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n",
      "\n",
      "\n",
      "\n",
      "It's also its most populous city!\n",
      "The capital of France is **Paris**.\n",
      "\n",
      "\n",
      "\n",
      "It's also its most populous city!\n"
     ]
    }
   ],
   "source": [
    "model_list = ollama_manager.list_models()\n",
    "print(model_list)\n",
    "\n",
    "# response = ollama_manager.generate_response(\"Hello, how are you?\", model=\"qwen3:4b\")\n",
    "# # print(json.dumps(response, indent=2))\n",
    "# print(response.json()['response'])\n",
    "\n",
    "# response = ollama_manager.generate_response(\"What is the capital of France?\", model=\"qwen3:4b\")\n",
    "# print(response.json()['response'])\n",
    "\n",
    "response = ollama_manager.generate_response(\"Hello, how are you?\", model=\"llama3.1:8b\")\n",
    "if response and 'response' in response:\n",
    "\tprint(response['response'])\n",
    "\n",
    "response = ollama_manager.generate_response(\"What is the capital of France?\", model=\"llama3.1:8b\")\n",
    "if response and 'response' in response:\n",
    "\tprint(response['response'])\n",
    "\n",
    "response = ollama_manager.generate_response(\"Hello, how are you?\", model=\"gemma3:12b\")\n",
    "if response and 'response' in response:\n",
    "\tprint(response['response'])\n",
    "\n",
    "response = ollama_manager.generate_response(\"What is the capital of France?\", model=\"gemma3:12b\")\n",
    "if response and 'response' in response:\n",
    "\tprint(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d0dbad",
   "metadata": {},
   "source": [
    "Collecting workspace informationBased on the error message and your notebook output, the issue with `qwen3:4b` getting HTTP 500 errors while `gemma3:4b` works fine is likely due to several factors:\n",
    "\n",
    "## 1. **Model Size and Resource Requirements**\n",
    "\n",
    "The `qwen3:4b` model may have different resource requirements than `gemma3:4b`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c474fdc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mcheck_model_info\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m model_list:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     result = \u001b[43msubprocess\u001b[49m.run([\u001b[33m\"\u001b[39m\u001b[33mollama\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mshow\u001b[39m\u001b[33m\"\u001b[39m, model], \n\u001b[32m     17\u001b[39m                           capture_output=\u001b[38;5;28;01mTrue\u001b[39;00m, text=\u001b[38;5;28;01mTrue\u001b[39;00m, check=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m model info:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'subprocess' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m subprocess.CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     22\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError getting model info: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mcheck_model_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mcheck_model_info\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m model info:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m         \u001b[38;5;28mprint\u001b[39m(result.stdout)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43msubprocess\u001b[49m.CalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError getting model info: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'subprocess' is not defined"
     ]
    }
   ],
   "source": [
    "# Check model sizes and resource usage\n",
    "def check_model_info():\n",
    "    \"\"\"Check detailed model information\"\"\"\n",
    "    try:\n",
    "        # Get detailed model info\n",
    "        model_list = [\n",
    "            \"gemma3:12b\",\n",
    "            # \"qwen3:4b\",\n",
    "            # \"gemma3:4b\",\n",
    "            # \"deepseek-r1:1.5b\",\n",
    "            # \"qwen3:8b\",\n",
    "            # \"mistral:7b\",\n",
    "            \"llama3.1:8b\",\n",
    "                      ]\n",
    "        for model in model_list:\n",
    "            result = subprocess.run([\"ollama\", \"show\", model], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"\\n{model} model info:\")\n",
    "            print(result.stdout)\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error getting model info: {e}\")\n",
    "\n",
    "check_model_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee29922",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. **Timeout Issues**\n",
    "\n",
    "The error shows a read timeout after 60 seconds. `qwen3:4b` might be slower to respond:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3c59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_longer_timeout(self, prompt, model=ollama_default_model, stream=False):\n",
    "    \"\"\"Generate response using Ollama API with extended timeout\"\"\"\n",
    "    url = f\"{self.base_url}/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": stream,\n",
    "        \"options\": {\n",
    "            \"num_predict\": 100,  # Limit tokens to speed up\n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Use longer timeout for qwen models\n",
    "    timeout = 180 if \"qwen\" in model.lower() else 60\n",
    "    \n",
    "    try:\n",
    "        print(f\"Generating with {model} (timeout: {timeout}s)...\")\n",
    "        response = requests.post(url, json=payload, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Timeout after {timeout} seconds with model {model}\")\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None\n",
    "\n",
    "# Update your OllamaManager class\n",
    "ollama_manager.generate_response = generate_response_with_longer_timeout.__get__(ollama_manager, OllamaManager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b21591",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. **Model Loading State**\n",
    "\n",
    "`qwen3:4b` might not be fully loaded into memory:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2347ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error checking model status: name 'requests' is not defined\n"
     ]
    }
   ],
   "source": [
    "def check_model_status():\n",
    "    \"\"\"Check if models are loaded and ready\"\"\"\n",
    "    try:\n",
    "        # Check running models/processes\n",
    "        response = requests.get('http://localhost:11434/api/ps')\n",
    "        if response.status_code == 200:\n",
    "            running_models = response.json()\n",
    "            print(\"Currently running models:\")\n",
    "            print(json.dumps(running_models, indent=2))\n",
    "        \n",
    "        # Test both models with simple prompts\n",
    "        models_to_test = [\n",
    "                            \"gemma3:12b\",\n",
    "                            # \"qwen3:4b\",\n",
    "                            # \"gemma3:4b\",\n",
    "                            # \"deepseek-r1:1.5b\",\n",
    "                            # \"qwen3:8b\",\n",
    "                            # \"mistral:7b\",\n",
    "                            \"llama3.1:8b\",\n",
    "                ]\n",
    "        \n",
    "        for model in models_to_test:\n",
    "            print(f\"\\nTesting {model}...\")\n",
    "            try:\n",
    "                response = requests.post('http://localhost:11434/api/generate', \n",
    "                                       json={\n",
    "                                           \"model\": model,\n",
    "                                           \"prompt\": \"Hello\",\n",
    "                                           \"stream\": False,\n",
    "                                           \"options\": {\"num_predict\": 10}\n",
    "                                       }, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    print(f\"✅ {model} responded: {result.get('response', '')}...\")\n",
    "                    # print(f\"✅ {model} responded: {result.get('response', '')[:50]}...\")\n",
    "                else:\n",
    "                    print(f\"❌ {model} failed with status: {response.status_code}\")\n",
    "                    print(f\"Error: {response.text}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"❌ {model} error: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error checking model status: {e}\")\n",
    "\n",
    "check_model_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c5cdcf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. **Memory and Performance Issues**\n",
    "\n",
    "Your system might struggle with `qwen3:4b`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a0a3150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 15.8 GB\n",
      "Available RAM: 8.7 GB\n",
      "Used RAM: 7.1 GB (45.0%)\n",
      "Ollama process: PID 21548, Memory: 47.5 MB\n",
      "Ollama process: PID 27512, Memory: 16.5 MB\n"
     ]
    }
   ],
   "source": [
    "def check_system_resources():\n",
    "    \"\"\"Check system resources and Ollama memory usage\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        \n",
    "        # Check memory usage\n",
    "        memory = psutil.virtual_memory()\n",
    "        print(f\"Total RAM: {memory.total / (1024**3):.1f} GB\")\n",
    "        print(f\"Available RAM: {memory.available / (1024**3):.1f} GB\")\n",
    "        print(f\"Used RAM: {memory.used / (1024**3):.1f} GB ({memory.percent}%)\")\n",
    "        \n",
    "        # Check for Ollama processes\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'memory_info']):\n",
    "            if 'ollama' in proc.info['name'].lower():\n",
    "                memory_mb = proc.info['memory_info'].rss / (1024 * 1024)\n",
    "                print(f\"Ollama process: PID {proc.info['pid']}, Memory: {memory_mb:.1f} MB\")\n",
    "                \n",
    "    except ImportError:\n",
    "        print(\"Install psutil for detailed system monitoring: pip install psutil\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking resources: {e}\")\n",
    "\n",
    "check_system_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6feb80",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. **Fix: Update Flask API with Better Error Handling**\n",
    "\n",
    "Update your Flask API to handle these model-specific issues:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d8f93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129m@app\u001b[39m\u001b[43m.\u001b[49m\u001b[43mroute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/api/generate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;129m@require_api_key\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapi_generate\u001b[39m():\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generate text using Ollama with model-specific handling\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m     data = request.get_json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\scaffold.py:46\u001b[39m, in \u001b[36msetupmethod.<locals>.wrapper_func\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_func\u001b[39m(\u001b[38;5;28mself\u001b[39m: Scaffold, *args: t.Any, **kwargs: t.Any) -> t.Any:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_setup_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\app.py:415\u001b[39m, in \u001b[36mApp._check_setup_finished\u001b[39m\u001b[34m(self, f_name)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_setup_finished\u001b[39m(\u001b[38;5;28mself\u001b[39m, f_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._got_first_request:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    416\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe setup method \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m can no longer be called\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m on the application. It has already handled its first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m request, any changes will not be applied\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m consistently.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    420\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMake sure all imports, decorators, functions, etc.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m needed to set up the application are done before\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m running it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    423\u001b[39m         )\n",
      "\u001b[31mAssertionError\u001b[39m: The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it."
     ]
    }
   ],
   "source": [
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama with model-specific handling\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', ollama_default_model)\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    # Model-specific timeout and options\n",
    "    if \"qwen\" in model.lower():\n",
    "        timeout = 180\n",
    "        options = {\"num_predict\": 100, \"temperature\": 0.7}\n",
    "    else:\n",
    "        timeout = 60\n",
    "        options = {\"num_predict\": 150, \"temperature\": 0.7}\n",
    "    \n",
    "    try:\n",
    "        # Direct Ollama API call with custom timeout\n",
    "        response = requests.post(f'http://localhost:11434/api/generate', \n",
    "                               json={\n",
    "                                   \"model\": model,\n",
    "                                   \"prompt\": prompt,\n",
    "                                   \"stream\": stream,\n",
    "                                   \"options\": options\n",
    "                               }, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return jsonify(response.json())\n",
    "        else:\n",
    "            return jsonify({\n",
    "                'error': f'Ollama API error: {response.status_code}',\n",
    "                'details': response.text\n",
    "            }), 500\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        return jsonify({\n",
    "            'error': f'Request timeout after {timeout} seconds with model {model}',\n",
    "            'suggestion': 'Try using a smaller model or reduce the prompt length'\n",
    "        }), 504\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'error': f'Failed to generate response: {str(e)}',\n",
    "            'model': model\n",
    "        }), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53452f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. **Recommended Solutions**\n",
    "\n",
    "1. **Use a smaller Qwen model**:\n",
    "   ```python\n",
    "   ollama_manager.pull_gemma_model(\"qwen2:1.5b\")  # Smaller, faster\n",
    "   ```\n",
    "\n",
    "2. **Increase timeouts in your Oxygen Positron configuration**\n",
    "\n",
    "3. **Pre-load the model** by running a simple query first:\n",
    "   ```python\n",
    "   # Warm up the model\n",
    "   ollama_manager.generate_response(\"Hi\", model=\"qwen3:4b\")\n",
    "   ```\n",
    "\n",
    "4. **Monitor resource usage** and consider using `gemma3:1b` or `qwen2:1.5b` for better performance on your system.\n",
    "\n",
    "The issue is likely that `qwen3:4b` requires more computational resources and time to respond, causing timeouts in your Flask API and Oxygen Positron integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea3d44",
   "metadata": {},
   "source": [
    "Looking at your code, you have an Intel GPU setup but are asking about NVIDIA MX250. The NVIDIA MX250 is a low-end discrete GPU that can potentially accelerate Ollama inference. Here's how to set it up:\n",
    "\n",
    "## 1. Check NVIDIA GPU Support\n",
    "\n",
    "First, verify your MX250 is detected and CUDA-capable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff61826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GPU detected:\n",
      "Wed Jun  4 00:33:59 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.52                 Driver Version: 576.52         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce MX250         WDDM  |   00000000:2B:00.0 Off |                  N/A |\n",
      "| N/A   48C    P0            N/A  / 5001W |       0MiB /   2048MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "\n",
      "CUDA Toolkit detected:\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Wed_Apr__9_19:29:17_Pacific_Daylight_Time_2025\n",
      "Cuda compilation tools, release 12.9, V12.9.41\n",
      "Build cuda_12.9.r12.9/compiler.35813241_0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def check_nvidia_gpu():\n",
    "    \"\"\"Check NVIDIA GPU status and CUDA support\"\"\"\n",
    "    try:\n",
    "        # Check nvidia-smi\n",
    "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"NVIDIA GPU detected:\")\n",
    "            print(result.stdout)\n",
    "        else:\n",
    "            print(\"NVIDIA GPU not detected or drivers not installed\")\n",
    "            return False\n",
    "            \n",
    "        # Check CUDA version\n",
    "        try:\n",
    "            result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "            if result.returncode == 0:\n",
    "                print(\"\\nCUDA Toolkit detected:\")\n",
    "                print(result.stdout)\n",
    "            else:\n",
    "                print(\"CUDA Toolkit not found\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"CUDA Toolkit not installed\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"NVIDIA drivers not installed\")\n",
    "        return False\n",
    "\n",
    "check_nvidia_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2caa8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Install CUDA Toolkit (if needed)\n",
    "\n",
    "If CUDA isn't installed:\n",
    "\n",
    "1. Download CUDA Toolkit from [NVIDIA's website](https://developer.nvidia.com/cuda-toolkit)\n",
    "2. Install the version compatible with your MX250 (CUDA 10.x or 11.x should work)\n",
    "3. Restart your system\n",
    "\n",
    "## 3. Configure Ollama for NVIDIA GPU\n",
    "\n",
    "Update your OllamaManager to utilize GPU acceleration:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9112ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        self.setup_gpu_environment()\n",
    "        \n",
    "    def setup_gpu_environment(self):\n",
    "        \"\"\"Setup environment variables for GPU acceleration\"\"\"\n",
    "        import os\n",
    "        \n",
    "        # Force NVIDIA GPU usage\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU\n",
    "        os.environ['OLLAMA_GPU_LAYERS'] = '35'    # Number of layers to offload to GPU\n",
    "        \n",
    "        # For MX250 (limited VRAM), be conservative\n",
    "        os.environ['OLLAMA_GPU_MEMORY'] = '1GB'   # Limit GPU memory usage\n",
    "        \n",
    "        print(\"GPU environment configured for NVIDIA MX250\")\n",
    "    \n",
    "    def check_gpu_acceleration(self):\n",
    "        \"\"\"Check if Ollama is using GPU acceleration\"\"\"\n",
    "        try:\n",
    "            # Check Ollama GPU status\n",
    "            response = requests.get(f\"{self.base_url}/api/ps\")\n",
    "            if response.status_code == 200:\n",
    "                processes = response.json()\n",
    "                print(\"Ollama processes:\")\n",
    "                print(json.dumps(processes, indent=2))\n",
    "                \n",
    "            # Test GPU usage with a model\n",
    "            test_response = self.generate_response(\"Hello\", model=\"gemma3:1b\")\n",
    "            if test_response:\n",
    "                print(\"✅ GPU acceleration test successful\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"❌ GPU acceleration test failed\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking GPU acceleration: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def generate_response(self, prompt, model=ollama_default_model, stream=False):\n",
    "        \"\"\"Generate response using Ollama API with GPU optimization\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        \n",
    "        # Optimize for MX250's limited VRAM\n",
    "        gpu_options = {\n",
    "            \"num_predict\": 1000,      # Reasonable token limit\n",
    "            \"temperature\": 0.7,\n",
    "            \"num_ctx\": 2048,          # Context window size\n",
    "            \"num_gpu\": 35,            # GPU layers for MX250\n",
    "            \"num_thread\": 4,          # CPU threads\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            \"options\": gpu_options\n",
    "        }\n",
    "        \n",
    "        # Adjust timeout based on model and GPU usage\n",
    "        if \"qwen\" in model.lower():\n",
    "            timeout = 120  # Reduced with GPU\n",
    "        elif \"gemma3:12b\" in model.lower():\n",
    "            timeout = 180  # Still large model\n",
    "        else:\n",
    "            timeout = 45   # Faster with GPU\n",
    "        \n",
    "        try:\n",
    "            print(f\"Generating with {model} on GPU (timeout: {timeout}s)...\")\n",
    "            response = requests.post(url, json=payload, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            if 'response' in result:\n",
    "                print(f\"GPU generation completed: {len(result['response'])} chars\")\n",
    "                # Don't print full response to avoid clutter\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"GPU timeout after {timeout} seconds with model {model}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"GPU generation error: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b10ef",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4. Start Ollama with GPU Support\n",
    "\n",
    "Modify your setup function to enable GPU:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95308d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_start():\n",
    "    \"\"\"Setup Ollama with GPU acceleration and start the Flask server\"\"\"\n",
    "    print(\"Setting up Ollama with NVIDIA MX250 GPU acceleration...\")\n",
    "    \n",
    "    # Setup GPU environment\n",
    "    import os\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    os.environ['OLLAMA_GPU_LAYERS'] = '35'\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    \n",
    "    # Install Ollama\n",
    "    if not ollama_manager.install_ollama():\n",
    "        print(\"⚠️  Please install Ollama manually\")\n",
    "        print(\"Then run: ollama serve\")\n",
    "        return None\n",
    "    \n",
    "    # Check GPU acceleration\n",
    "    print(\"\\n🔍 Checking GPU acceleration...\")\n",
    "    gpu_working = ollama_manager.check_gpu_acceleration()\n",
    "    \n",
    "    if gpu_working:\n",
    "        print(\"✅ NVIDIA MX250 GPU acceleration is working!\")\n",
    "    else:\n",
    "        print(\"⚠️  GPU acceleration may not be working. Check CUDA installation.\")\n",
    "    \n",
    "    # Pull optimized models for MX250\n",
    "    recommended_models = [\"gemma3:1b\", \"gemma3:4b\"]  # Start with smaller models\n",
    "    for model in recommended_models:\n",
    "        print(f\"\\n📥 Pulling {model} (optimized for MX250)...\")\n",
    "        ollama_manager.pull_gemma_model(model)\n",
    "    \n",
    "    # List available models\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        print(\"\\n📋 Available models:\")\n",
    "        print(models)\n",
    "    \n",
    "    # Start Flask server\n",
    "    start_server()\n",
    "    \n",
    "    print(f\"\\n✅ Server setup complete with GPU acceleration!\")\n",
    "    print(f\"🌐 Health check: http://localhost:5000/health\")\n",
    "    print(f\"🎮 GPU Status: {'Enabled' if gpu_working else 'Check CUDA installation'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8ae65",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 5. Monitor GPU Usage\n",
    "\n",
    "Add GPU monitoring to your health check:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5eff91",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;129m@app\u001b[39m\u001b[43m.\u001b[49m\u001b[43mroute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/health\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethods\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhealth_check\u001b[39m():\n\u001b[32m      3\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Enhanced health check with GPU monitoring\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m         \u001b[38;5;66;03m# Check Ollama connectivity\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\scaffold.py:46\u001b[39m, in \u001b[36msetupmethod.<locals>.wrapper_func\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper_func\u001b[39m(\u001b[38;5;28mself\u001b[39m: Scaffold, *args: t.Any, **kwargs: t.Any) -> t.Any:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_setup_finished\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Project\\OCR\\.venv\\Lib\\site-packages\\flask\\sansio\\app.py:415\u001b[39m, in \u001b[36mApp._check_setup_finished\u001b[39m\u001b[34m(self, f_name)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_setup_finished\u001b[39m(\u001b[38;5;28mself\u001b[39m, f_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._got_first_request:\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    416\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe setup method \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m can no longer be called\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m on the application. It has already handled its first\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m request, any changes will not be applied\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    419\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m consistently.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    420\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMake sure all imports, decorators, functions, etc.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    421\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m needed to set up the application are done before\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    422\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m running it.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    423\u001b[39m         )\n",
      "\u001b[31mAssertionError\u001b[39m: The setup method 'route' can no longer be called on the application. It has already handled its first request, any changes will not be applied consistently.\nMake sure all imports, decorators, functions, etc. needed to set up the application are done before running it."
     ]
    }
   ],
   "source": [
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check with GPU monitoring\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        gpu_status = \"unknown\"\n",
    "        \n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "            \n",
    "            # Check GPU usage\n",
    "            gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', \n",
    "                                       '--format=csv,noheader,nounits'], \n",
    "                                      capture_output=True, text=True)\n",
    "            if gpu_result.returncode == 0:\n",
    "                gpu_info = gpu_result.stdout.strip().split(', ')\n",
    "                gpu_status = f\"GPU: {gpu_info[0]}% util, {gpu_info[1]}MB/{gpu_info[2]}MB memory\"\n",
    "            else:\n",
    "                gpu_status = \"GPU not available\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            gpu_status = f\"GPU check failed: {e}\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status,\n",
    "                'gpu': gpu_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85190bbb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 6. Test GPU Acceleration\n",
    "\n",
    "Add a test function to verify GPU is working:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7decd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:11434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing llama3.1:8b performance...\n",
      "Generating with llama3.1:8b (timeout: 180s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://localhost:11434 \"POST /api/generate HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a complex topic, but I'll try to break it down in simple terms.\n",
      "\n",
      "**What's the problem with regular computers?**\n",
      "\n",
      "Regular computers use \"bits\" to store and process information. A bit can only be 0 or 1, like a light switch that's either on (1) or off (0). This binary system is great for most tasks, but it has limitations when dealing with complex problems.\n",
      "\n",
      "**Enter quantum computing:**\n",
      "\n",
      "Quantum computers use \"qubits\" (quantum bits), which are special kinds of particles that can exist in multiple states simultaneously. Think of a coin that's both heads and tails at the same time!\n",
      "\n",
      "Qubits have three main features:\n",
      "\n",
      "1. **Superposition**: Qubits can represent 0, 1, or any combination of both (like 50% heads and 50% tails).\n",
      "2. **Entanglement**: Qubits can be connected in a way that affects each other instantly, even if they're on opposite sides of the universe.\n",
      "3. **Quantum fluctuations**: Qubits can exist in multiple states at the same time due to random quantum fluctuations.\n",
      "\n",
      "**How does this help?**\n",
      "\n",
      "With qubits, quantum computers can:\n",
      "\n",
      "1. **Process vast amounts of data simultaneously**: By existing in multiple states, qubits can explore an enormous number of possibilities in parallel.\n",
      "2. **Solve complex problems more efficiently**: Quantum computers can tackle problems that are too difficult or time-consuming for regular computers.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "* **Cryptography**: Quantum computers can break certain encryption codes used to protect online transactions and data. However, they could also create unbreakable quantum encryption methods!\n",
      "* **Optimization**: Quantum computers can quickly find the best solution among an enormous number of possibilities, which is useful in fields like logistics, finance, or energy management.\n",
      "* **Simulation**: Quantum computers can simulate complex systems, such as molecules or materials, which could lead to breakthroughs in chemistry and physics.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "While quantum computing has immense potential, it's still a developing field. Some challenges include:\n",
      "\n",
      "* **Error correction**: Qubits are prone to errors due to their fragile nature.\n",
      "* **Scalability**: Currently, most quantum computers can only process a limited number of qubits.\n",
      "* **Quantum noise**: The fragile state of qubits makes them susceptible to external interference.\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "In simple terms, quantum computing is like a super-powerful calculator that can solve complex problems more efficiently by using special particles called qubits. While it's still in its early stages, the potential for breakthroughs and innovation is vast!\n",
      "\n",
      "⏱️  GPU Generation Time: 117.00 seconds\n",
      "📝 Response length: 2597 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "116.99719643592834"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_gpu_performance():\n",
    "    \"\"\"Test GPU vs CPU performance\"\"\"\n",
    "    import time\n",
    "    \n",
    "    test_prompt = \"Explain quantum computing in simple terms.\"\n",
    "    # model = \"gemma3:1b\"  # Use small model for testing\n",
    "    model = \"llama3.1:8b\"  # Use small model for testing\n",
    "    \n",
    "    print(f\"Testing {model} performance...\")\n",
    "    \n",
    "    # Test with GPU\n",
    "    start_time = time.time()\n",
    "    gpu_response = ollama_manager.generate_response(test_prompt, model)\n",
    "    gpu_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n⏱️  GPU Generation Time: {gpu_time:.2f} seconds\")\n",
    "    if gpu_response and 'response' in gpu_response:\n",
    "        print(f\"📝 Response length: {len(gpu_response['response'])} characters\")\n",
    "    \n",
    "    return gpu_time\n",
    "\n",
    "# Run the test\n",
    "test_gpu_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da52399",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Important Notes for MX250:\n",
    "\n",
    "1. **Limited VRAM**: MX250 has only 2GB VRAM, so use smaller models (1B-4B parameters)\n",
    "2. **Power Efficiency**: MX250 is designed for laptops, so it may throttle under heavy load\n",
    "3. **Model Recommendations**: Start with `gemma3:1b` and `gemma3:4b` before trying larger models\n",
    "4. **Memory Management**: The GPU memory limit settings are crucial to avoid crashes\n",
    "\n",
    "The MX250 should provide modest acceleration compared to CPU-only inference, especially for smaller models. Monitor GPU utilization to ensure it's being used effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ee5fa",
   "metadata": {},
   "source": [
    "# Version 3.0 with GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97036f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Ollama and Gemma model...\n",
      "Ollama is already installed\n",
      "Available models:\n",
      "NAME           ID              SIZE      MODIFIED     \n",
      "llama3.1:8b    46e0c10c039e    4.9 GB    24 hours ago    \n",
      "gemma3:12b     f4031aab637d    8.1 GB    26 hours ago    \n",
      "\n",
      "Flask server started on http://localhost:5000\n",
      " * Serving Flask app '__main__'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.48.93:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import hashlib\n",
    "import time\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from functools import wraps\n",
    "from flask import Flask, request, jsonify, g\n",
    "import requests\n",
    "import sqlite3\n",
    "import logging\n",
    "\n",
    "ollama_default_model = \"gemma3:1b\"\n",
    "\n",
    "# Install required packages\n",
    "def install_packages():\n",
    "    packages = ['flask', 'requests']\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "        except ImportError:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_packages()\n",
    "\n",
    "# 1. Ollama Setup and Gemma Model Management\n",
    "class OllamaManager:\n",
    "    def __init__(self, base_url=\"http://localhost:11434\"):\n",
    "        self.base_url = base_url\n",
    "        \n",
    "    def install_ollama(self):\n",
    "        \"\"\"Install Ollama if not already installed\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"--version\"], check=True, capture_output=True, text=True)\n",
    "            print(\"Ollama is already installed\")\n",
    "            return True\n",
    "        except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "            print(\"Installing Ollama...\")\n",
    "            # For Windows\n",
    "            if os.name == 'nt':\n",
    "                print(\"Please download and install Ollama from: https://ollama.ai/download\")\n",
    "                print(\"After installation, restart this notebook and run 'ollama serve' in a terminal\")\n",
    "                return False\n",
    "            else:\n",
    "                try:\n",
    "                    # Download installer script\n",
    "                    import urllib.request\n",
    "                    urllib.request.urlretrieve(\"https://ollama.ai/install.sh\", \"ollama_install.sh\")\n",
    "                    subprocess.run([\"chmod\", \"+x\", \"ollama_install.sh\"], check=True)\n",
    "                    subprocess.run([\"./ollama_install.sh\"], check=True)\n",
    "                    print(\"Ollama installed successfully\")\n",
    "                    return True\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to install Ollama: {e}\")\n",
    "                    return False\n",
    "    \n",
    "    def pull_gemma_model(self, model_name=ollama_default_model):\n",
    "        \"\"\"Pull Gemma model\"\"\"\n",
    "        try:\n",
    "            print(f\"Pulling {model_name} model...\")\n",
    "            result = subprocess.run([\"ollama\", \"pull\", model_name], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            print(f\"Successfully pulled {model_name}\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error pulling model: {e}\")\n",
    "            print(f\"Make sure Ollama is running with 'ollama serve'\")\n",
    "            return False\n",
    "    \n",
    "    def list_models(self):\n",
    "        \"\"\"List available models\"\"\"\n",
    "        try:\n",
    "            result = subprocess.run([\"ollama\", \"list\"], \n",
    "                                  capture_output=True, text=True, check=True)\n",
    "            return result.stdout\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error listing models: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_models(self, models_output):\n",
    "        \"\"\"Parse ollama list output into structured format\"\"\"\n",
    "        if not models_output:\n",
    "            return []\n",
    "        \n",
    "        models = []\n",
    "        lines = models_output.strip().split('\\n')[1:]  # Skip header\n",
    "        for line in lines:\n",
    "            if line.strip():\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 3:\n",
    "                    model_name = parts[0]\n",
    "                    models.append({\n",
    "                        \"id\": model_name,\n",
    "                        \"object\": \"model\",\n",
    "                        \"created\": int(time.time()),\n",
    "                        \"owned_by\": \"local\"\n",
    "                    })\n",
    "        return models\n",
    "    \n",
    "    def generate_response(self, prompt, model=ollama_default_model, stream=False):\n",
    "        \"\"\"Generate response using Ollama API with extended timeout\"\"\"\n",
    "        url = f\"{self.base_url}/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": stream,\n",
    "            \"options\": {\n",
    "                \"num_predict\": 3000,  # Limit tokens to speed up\n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Use longer timeout for qwen models\n",
    "        if \"qwen\" in model.lower():\n",
    "            timeout = 480 \n",
    "        elif \"deepseek\" in model.lower() or \"llama\" in model.lower() or \"mistral\" in model.lower():\n",
    "            timeout = 180 \n",
    "        elif \"gemma3:12b\" in model.lower():\n",
    "            timeout = 360 \n",
    "        else:\n",
    "            timeout = 60\n",
    "        # timeout = 480 if \"qwen\" in model.lower() or \"deepseek\" in model.lower() else 60\n",
    "        \n",
    "        try:\n",
    "            print(f\"Generating with {model} (timeout: {timeout}s)...\")\n",
    "            response = requests.post(url, json=payload, timeout=timeout)\n",
    "            response.raise_for_status()\n",
    "            # print(json.dumps(response.json(), indent=2))\n",
    "            result = response.json()\n",
    "            if 'response' in result:\n",
    "                print(result['response'])\n",
    "            else:\n",
    "                print(json.dumps(result, indent=2))\n",
    "            return response.json()\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout after {timeout} seconds with model {model}\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return None\n",
    "\n",
    "# 2. API Key Manager\n",
    "class APIKeyManager:\n",
    "    def __init__(self, db_path=\"api_keys.db\"):\n",
    "        self.db_path = db_path\n",
    "        self.init_database()\n",
    "    \n",
    "    def init_database(self):\n",
    "        \"\"\"Initialize SQLite database for API keys\"\"\"\n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS api_keys (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                key_hash TEXT UNIQUE NOT NULL,\n",
    "                name TEXT NOT NULL,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                last_used TIMESTAMP,\n",
    "                is_active BOOLEAN DEFAULT 1,\n",
    "                usage_count INTEGER DEFAULT 0,\n",
    "                rate_limit INTEGER DEFAULT 100\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def generate_api_key(self, name):\n",
    "        \"\"\"Generate a new API key\"\"\"\n",
    "        api_key = f\"sk-{uuid.uuid4().hex}\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        try:\n",
    "            cursor.execute('''\n",
    "                INSERT INTO api_keys (key_hash, name) VALUES (?, ?)\n",
    "            ''', (key_hash, name))\n",
    "            conn.commit()\n",
    "            return api_key\n",
    "        except sqlite3.IntegrityError:\n",
    "            return None\n",
    "        finally:\n",
    "            conn.close()\n",
    "    \n",
    "    def validate_api_key(self, api_key):\n",
    "        \"\"\"Validate API key and update usage\"\"\"\n",
    "        key_hash = hashlib.sha256(api_key.encode()).hexdigest()\n",
    "        \n",
    "        conn = sqlite3.connect(self.db_path)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            SELECT id, is_active, usage_count, rate_limit FROM api_keys \n",
    "            WHERE key_hash = ?\n",
    "        ''', (key_hash,))\n",
    "        \n",
    "        result = cursor.fetchone()\n",
    "        if result and result[1]:  # is_active\n",
    "            # Update last_used and usage_count\n",
    "            cursor.execute('''\n",
    "                UPDATE api_keys \n",
    "                SET last_used = CURRENT_TIMESTAMP, usage_count = usage_count + 1\n",
    "                WHERE key_hash = ?\n",
    "            ''', (key_hash,))\n",
    "            conn.commit()\n",
    "            conn.close()\n",
    "            return True\n",
    "        \n",
    "        conn.close()\n",
    "        return False\n",
    "\n",
    "# 3. Flask API Wrapper\n",
    "app = Flask(__name__)\n",
    "ollama_manager = OllamaManager()\n",
    "api_key_manager = APIKeyManager()\n",
    "\n",
    "def require_api_key(f):\n",
    "    \"\"\"Decorator to require API key authentication\"\"\"\n",
    "    @wraps(f)\n",
    "    def decorated_function(*args, **kwargs):\n",
    "        api_key = request.headers.get('Authorization')\n",
    "        if not api_key:\n",
    "            return jsonify({'error': 'API key required'}), 401\n",
    "        \n",
    "        if api_key.startswith('Bearer '):\n",
    "            api_key = api_key[7:]\n",
    "        \n",
    "        if not api_key_manager.validate_api_key(api_key):\n",
    "            return jsonify({'error': 'Invalid API key'}), 401\n",
    "        \n",
    "        return f(*args, **kwargs)\n",
    "    return decorated_function\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    \"\"\"Enhanced health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Check Ollama connectivity\n",
    "        ollama_status = \"unknown\"\n",
    "        try:\n",
    "            models = ollama_manager.list_models()\n",
    "            ollama_status = \"healthy\" if models else \"unhealthy\"\n",
    "        except:\n",
    "            ollama_status = \"unhealthy\"\n",
    "        \n",
    "        # Check database connectivity\n",
    "        db_status = \"unknown\"\n",
    "        try:\n",
    "            conn = sqlite3.connect(api_key_manager.db_path)\n",
    "            conn.close()\n",
    "            db_status = \"healthy\"\n",
    "        except:\n",
    "            db_status = \"unhealthy\"\n",
    "        \n",
    "        overall_status = \"healthy\" if ollama_status == \"healthy\" and db_status == \"healthy\" else \"degraded\"\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': overall_status,\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'services': {\n",
    "                'ollama': ollama_status,\n",
    "                'database': db_status\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({\n",
    "            'status': 'unhealthy',\n",
    "            'timestamp': datetime.utcnow().isoformat(),\n",
    "            'error': str(e)\n",
    "        }), 500\n",
    "\n",
    "@app.route('/api/generate', methods=['POST'])\n",
    "@require_api_key\n",
    "def api_generate():\n",
    "    \"\"\"Generate text using Ollama\"\"\"\n",
    "    data = request.get_json()\n",
    "    \n",
    "    if not data or 'prompt' not in data:\n",
    "        return jsonify({'error': 'Prompt is required'}), 400\n",
    "    \n",
    "    prompt = data['prompt']\n",
    "    model = data.get('model', 'gemma3:4b')\n",
    "    stream = data.get('stream', False)\n",
    "    \n",
    "    response = ollama_manager.generate_response(prompt, model, stream)\n",
    "    \n",
    "    if response:\n",
    "        return jsonify(response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/api/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def api_list_models():\n",
    "    \"\"\"List available models\"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        return jsonify({'models': parsed_models})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "@app.route('/api/keys/generate', methods=['POST'])\n",
    "def generate_key():\n",
    "    \"\"\"Generate new API key (admin endpoint)\"\"\"\n",
    "    data = request.get_json()\n",
    "    if not data or 'name' not in data:\n",
    "        return jsonify({'error': 'Name is required'}), 400\n",
    "    \n",
    "    api_key = api_key_manager.generate_api_key(data['name'])\n",
    "    if api_key:\n",
    "        return jsonify({'api_key': api_key})\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate API key'}), 500\n",
    "\n",
    "# 4. Oxygen Positron Custom Connector\n",
    "@app.route('/ai/chat/completions', methods=['POST'])\n",
    "@require_api_key\n",
    "def oxygen_chat_completions():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible endpoint\n",
    "    Follows OpenAI Chat Completions API format`\n",
    "    \"\"\"\n",
    "    data = request.get_json()\n",
    "    print(f\"Received data: {data}\")\n",
    "    \n",
    "    if not data or 'messages' not in data:\n",
    "        return jsonify({'error': 'Messages are required'}), 400\n",
    "    \n",
    "    messages = data['messages']\n",
    "    model = data.get('model', ollama_default_model)\n",
    "    max_tokens = data.get('max_tokens', 150)\n",
    "    temperature = data.get('temperature', 0.7)\n",
    "    \n",
    "    # Convert messages to a single prompt\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role', 'user')\n",
    "        content = message.get('content', '')\n",
    "        if role == 'system':\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == 'user':\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == 'assistant':\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    # Generate response using Ollama\n",
    "    response = ollama_manager.generate_response(prompt, model)\n",
    "    \n",
    "    if response and 'response' in response:\n",
    "        # Format response in OpenAI Chat Completions format\n",
    "        completion_response = {\n",
    "            \"id\": f\"chatcmpl-{uuid.uuid4().hex[:8]}\",\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),\n",
    "            \"model\": model,\n",
    "            \"choices\": [{\n",
    "                \"index\": 0,\n",
    "                \"message\": {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['response']\n",
    "                },\n",
    "                \"finish_reason\": \"stop\"\n",
    "            }],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": len(response['response'].split()),\n",
    "                \"total_tokens\": len(prompt.split()) + len(response['response'].split())\n",
    "            }\n",
    "        }\n",
    "        return jsonify(completion_response)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to generate response'}), 500\n",
    "\n",
    "@app.route('/ai/models', methods=['GET'])\n",
    "@require_api_key\n",
    "def oxygen_list_models():\n",
    "    \"\"\"\n",
    "    Oxygen Positron compatible models endpoint\n",
    "    \"\"\"\n",
    "    models_output = ollama_manager.list_models()\n",
    "    if models_output:\n",
    "        # Parse and format models for Oxygen Positron\n",
    "        parsed_models = ollama_manager.parse_models(models_output)\n",
    "        if not parsed_models:\n",
    "            # Fallback if no models found\n",
    "            parsed_models = [{\n",
    "                \"id\": ollama_default_model,\n",
    "                \"object\": \"model\",\n",
    "                \"created\": int(time.time()),\n",
    "                \"owned_by\": \"local\"\n",
    "            }]\n",
    "        \n",
    "        model_list = {\n",
    "            \"object\": \"list\",\n",
    "            \"data\": parsed_models\n",
    "        }\n",
    "        return jsonify(model_list)\n",
    "    else:\n",
    "        return jsonify({'error': 'Failed to list models'}), 500\n",
    "\n",
    "# Global variable to track server thread\n",
    "server_thread = None\n",
    "\n",
    "def run_flask_app():\n",
    "    \"\"\"Run Flask app in a separate thread\"\"\"\n",
    "    # Enable Flask debug logging\n",
    "    logging.basicConfig(level=logging.DEBUG)\n",
    "    app.logger.setLevel(logging.DEBUG)\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)\n",
    "\n",
    "def start_server():\n",
    "    \"\"\"Start the Flask server\"\"\"\n",
    "    global server_thread\n",
    "    if server_thread is None or not server_thread.is_alive():\n",
    "        server_thread = threading.Thread(target=run_flask_app, daemon=True)\n",
    "        server_thread.start()\n",
    "        print(\"Flask server started on http://localhost:5000\")\n",
    "    else:\n",
    "        print(\"Server is already running\")\n",
    "\n",
    "def setup_and_start():\n",
    "    \"\"\"Setup Ollama and start the Flask server\"\"\"\n",
    "    print(\"Setting up Ollama and Gemma model...\")\n",
    "    \n",
    "    # Install Ollama\n",
    "    if not ollama_manager.install_ollama():\n",
    "        print(\"⚠️  Please install Ollama manually and run 'ollama serve' before continuing\")\n",
    "        return None\n",
    "    \n",
    "    # Try to list models (this will also test if Ollama is running)\n",
    "    models = ollama_manager.list_models()\n",
    "    if models:\n",
    "        print(\"Available models:\")\n",
    "        print(models)\n",
    "    else:\n",
    "        print(\"⚠️  No models found. You may need to pull a model first.\")\n",
    "        print(f\"Run: ollama pull {ollama_default_model}\")\n",
    "    \n",
    "    # Generate initial API key\n",
    "    # initial_key = api_key_manager.generate_api_key(\"default\")\n",
    "    # if initial_key:\n",
    "    #     print(f\"\\n🔑 Initial API Key: {initial_key}\")\n",
    "    #     print(\"Save this key - it won't be shown again!\")\n",
    "    \n",
    "    # Start Flask app in background thread\n",
    "    start_server()\n",
    "    \n",
    "    # return initial_key\n",
    "\n",
    "# Initialize setup\n",
    "if __name__ == '__main__':\n",
    "    setup_and_start()\n",
    "else:\n",
    "    # When imported in Jupyter, just run setup\n",
    "    initial_key = setup_and_start()\n",
    "    if initial_key:\n",
    "        print(f\"\\n✅ Server setup complete!\")\n",
    "        print(f\"🌐 Health check: http://localhost:5000/health\")\n",
    "        print(f\"📚 API documentation available in the notebook cells above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
