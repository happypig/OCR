<!-- Generated by SpecStory -->

# Utilizing NVIDIA MX250 with Ollama for Inference (2025-06-03 18:07:22)

_**User**_

how to utilize nVIdia MX250 with Ollama

---

_**Assistant**_

Looking at your code, you have an Intel GPU setup but are asking about NVIDIA MX250. The NVIDIA MX250 is a low-end discrete GPU that can potentially accelerate Ollama inference. Here's how to set it up:

## 1. Check NVIDIA GPU Support

First, verify your MX250 is detected and CUDA-capable:

````python
import subprocess
import json

def check_nvidia_gpu():
    """Check NVIDIA GPU status and CUDA support"""
    try:
        # Check nvidia-smi
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("NVIDIA GPU detected:")
            print(result.stdout)
        else:
            print("NVIDIA GPU not detected or drivers not installed")
            return False
            
        # Check CUDA version
        try:
            result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
            if result.returncode == 0:
                print("\nCUDA Toolkit detected:")
                print(result.stdout)
            else:
                print("CUDA Toolkit not found")
        except FileNotFoundError:
            print("CUDA Toolkit not installed")
            
        return True
        
    except FileNotFoundError:
        print("NVIDIA drivers not installed")
        return False

check_nvidia_gpu()
````

## 2. Install CUDA Toolkit (if needed)

If CUDA isn't installed:

1. Download CUDA Toolkit from [NVIDIA's website](https://developer.nvidia.com/cuda-toolkit)
2. Install the version compatible with your MX250 (CUDA 10.x or 11.x should work)
3. Restart your system

## 3. Configure Ollama for NVIDIA GPU

Update your OllamaManager to utilize GPU acceleration:

````python
class OllamaManager:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.setup_gpu_environment()
        
    def setup_gpu_environment(self):
        """Setup environment variables for GPU acceleration"""
        import os
        
        # Force NVIDIA GPU usage
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU
        os.environ['OLLAMA_GPU_LAYERS'] = '35'    # Number of layers to offload to GPU
        
        # For MX250 (limited VRAM), be conservative
        os.environ['OLLAMA_GPU_MEMORY'] = '1GB'   # Limit GPU memory usage
        
        print("GPU environment configured for NVIDIA MX250")
    
    def check_gpu_acceleration(self):
        """Check if Ollama is using GPU acceleration"""
        try:
            # Check Ollama GPU status
            response = requests.get(f"{self.base_url}/api/ps")
            if response.status_code == 200:
                processes = response.json()
                print("Ollama processes:")
                print(json.dumps(processes, indent=2))
                
            # Test GPU usage with a model
            test_response = self.generate_response("Hello", model="gemma3:1b")
            if test_response:
                print("‚úÖ GPU acceleration test successful")
                return True
            else:
                print("‚ùå GPU acceleration test failed")
                return False
                
        except Exception as e:
            print(f"Error checking GPU acceleration: {e}")
            return False
    
    def generate_response(self, prompt, model=ollama_default_model, stream=False):
        """Generate response using Ollama API with GPU optimization"""
        url = f"{self.base_url}/api/generate"
        
        # Optimize for MX250's limited VRAM
        gpu_options = {
            "num_predict": 1000,      # Reasonable token limit
            "temperature": 0.7,
            "num_ctx": 2048,          # Context window size
            "num_gpu": 35,            # GPU layers for MX250
            "num_thread": 4,          # CPU threads
        }
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": gpu_options
        }
        
        # Adjust timeout based on model and GPU usage
        if "qwen" in model.lower():
            timeout = 120  # Reduced with GPU
        elif "gemma3:12b" in model.lower():
            timeout = 180  # Still large model
        else:
            timeout = 45   # Faster with GPU
        
        try:
            print(f"Generating with {model} on GPU (timeout: {timeout}s)...")
            response = requests.post(url, json=payload, timeout=timeout)
            response.raise_for_status()
            result = response.json()
            
            if 'response' in result:
                print(f"GPU generation completed: {len(result['response'])} chars")
                # Don't print full response to avoid clutter
            
            return result
            
        except requests.exceptions.Timeout:
            print(f"GPU timeout after {timeout} seconds with model {model}")
            return None
        except requests.exceptions.RequestException as e:
            print(f"GPU generation error: {e}")
            return None
````

## 4. Start Ollama with GPU Support

Modify your setup function to enable GPU:

````python
def setup_and_start():
    """Setup Ollama with GPU acceleration and start the Flask server"""
    print("Setting up Ollama with NVIDIA MX250 GPU acceleration...")
    
    # Setup GPU environment
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    os.environ['OLLAMA_GPU_LAYERS'] = '35'
    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'
    
    # Install Ollama
    if not ollama_manager.install_ollama():
        print("‚ö†Ô∏è  Please install Ollama manually")
        print("Then run: ollama serve")
        return None
    
    # Check GPU acceleration
    print("\nüîç Checking GPU acceleration...")
    gpu_working = ollama_manager.check_gpu_acceleration()
    
    if gpu_working:
        print("‚úÖ NVIDIA MX250 GPU acceleration is working!")
    else:
        print("‚ö†Ô∏è  GPU acceleration may not be working. Check CUDA installation.")
    
    # Pull optimized models for MX250
    recommended_models = ["gemma3:1b", "gemma3:4b"]  # Start with smaller models
    for model in recommended_models:
        print(f"\nüì• Pulling {model} (optimized for MX250)...")
        ollama_manager.pull_gemma_model(model)
    
    # List available models
    models = ollama_manager.list_models()
    if models:
        print("\nüìã Available models:")
        print(models)
    
    # Start Flask server
    start_server()
    
    print(f"\n‚úÖ Server setup complete with GPU acceleration!")
    print(f"üåê Health check: http://localhost:5000/health")
    print(f"üéÆ GPU Status: {'Enabled' if gpu_working else 'Check CUDA installation'}")
````

## 5. Monitor GPU Usage

Add GPU monitoring to your health check:

````python
@app.route('/health', methods=['GET'])
def health_check():
    """Enhanced health check with GPU monitoring"""
    try:
        # Check Ollama connectivity
        ollama_status = "unknown"
        gpu_status = "unknown"
        
        try:
            models = ollama_manager.list_models()
            ollama_status = "healthy" if models else "unhealthy"
            
            # Check GPU usage
            gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', 
                                       '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True)
            if gpu_result.returncode == 0:
                gpu_info = gpu_result.stdout.strip().split(', ')
                gpu_status = f"GPU: {gpu_info[0]}% util, {gpu_info[1]}MB/{gpu_info[2]}MB memory"
            else:
                gpu_status = "GPU not available"
                
        except Exception as e:
            gpu_status = f"GPU check failed: {e}"
        
        # Check database connectivity
        db_status = "unknown"
        try:
            conn = sqlite3.connect(api_key_manager.db_path)
            conn.close()
            db_status = "healthy"
        except:
            db_status = "unhealthy"
        
        overall_status = "healthy" if ollama_status == "healthy" and db_status == "healthy" else "degraded"
        
        return jsonify({
            'status': overall_status,
            'timestamp': datetime.utcnow().isoformat(),
            'services': {
                'ollama': ollama_status,
                'database': db_status,
                'gpu': gpu_status
            }
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'timestamp': datetime.utcnow().isoformat(),
            'error': str(e)
        }), 500
````

## 6. Test GPU Acceleration

Add a test function to verify GPU is working:

````python
def test_gpu_performance():
    """Test GPU vs CPU performance"""
    import time
    
    test_prompt = "Explain quantum computing in simple terms."
    model = "gemma3:1b"  # Use small model for testing
    
    print(f"Testing {model} performance...")
    
    # Test with GPU
    start_time = time.time()
    gpu_response = ollama_manager.generate_response(test_prompt, model)
    gpu_time = time.time() - start_time
    
    print(f"\n‚è±Ô∏è  GPU Generation Time: {gpu_time:.2f} seconds")
    if gpu_response and 'response' in gpu_response:
        print(f"üìù Response length: {len(gpu_response['response'])} characters")
    
    return gpu_time

# Run the test
test_gpu_performance()
````

## Important Notes for MX250:

1. **Limited VRAM**: MX250 has only 2GB VRAM, so use smaller models (1B-4B parameters)
2. **Power Efficiency**: MX250 is designed for laptops, so it may throttle under heavy load
3. **Model Recommendations**: Start with `gemma3:1b` and `gemma3:4b` before trying larger models
4. **Memory Management**: The GPU memory limit settings are crucial to avoid crashes

The MX250 should provide modest acceleration compared to CPU-only inference, especially for smaller models. Monitor GPU utilization to ensure it's being used effectively.

---

