<!-- Generated by SpecStory -->

# Utilizing NVIDIA MX250 with Ollama for Inference (2025-06-03 18:07:22)

_**User**_

how to utilize nVIdia MX250 with Ollama

---

_**Assistant**_

Looking at your code, you have an Intel GPU setup but are asking about NVIDIA MX250. The NVIDIA MX250 is a low-end discrete GPU that can potentially accelerate Ollama inference. Here's how to set it up:

## 1. Check NVIDIA GPU Support

First, verify your MX250 is detected and CUDA-capable:

````python
import subprocess
import json

def check_nvidia_gpu():
    """Check NVIDIA GPU status and CUDA support"""
    try:
        # Check nvidia-smi
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("NVIDIA GPU detected:")
            print(result.stdout)
        else:
            print("NVIDIA GPU not detected or drivers not installed")
            return False
            
        # Check CUDA version
        try:
            result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
            if result.returncode == 0:
                print("\nCUDA Toolkit detected:")
                print(result.stdout)
            else:
                print("CUDA Toolkit not found")
        except FileNotFoundError:
            print("CUDA Toolkit not installed")
            
        return True
        
    except FileNotFoundError:
        print("NVIDIA drivers not installed")
        return False

check_nvidia_gpu()
````

## 2. Install CUDA Toolkit (if needed)

If CUDA isn't installed:

1. Download CUDA Toolkit from [NVIDIA's website](https://developer.nvidia.com/cuda-toolkit)
2. Install the version compatible with your MX250 (CUDA 10.x or 11.x should work)
3. Restart your system

## 3. Configure Ollama for NVIDIA GPU

Update your OllamaManager to utilize GPU acceleration:

````python
class OllamaManager:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.setup_gpu_environment()
        
    def setup_gpu_environment(self):
        """Setup environment variables for GPU acceleration"""
        import os
        
        # Force NVIDIA GPU usage
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # Use first GPU
        os.environ['OLLAMA_GPU_LAYERS'] = '35'    # Number of layers to offload to GPU
        
        # For MX250 (limited VRAM), be conservative
        os.environ['OLLAMA_GPU_MEMORY'] = '1GB'   # Limit GPU memory usage
        
        print("GPU environment configured for NVIDIA MX250")
    
    def check_gpu_acceleration(self):
        """Check if Ollama is using GPU acceleration"""
        try:
            # Check Ollama GPU status
            response = requests.get(f"{self.base_url}/api/ps")
            if response.status_code == 200:
                processes = response.json()
                print("Ollama processes:")
                print(json.dumps(processes, indent=2))
                
            # Test GPU usage with a model
            test_response = self.generate_response("Hello", model="gemma3:1b")
            if test_response:
                print("‚úÖ GPU acceleration test successful")
                return True
            else:
                print("‚ùå GPU acceleration test failed")
                return False
                
        except Exception as e:
            print(f"Error checking GPU acceleration: {e}")
            return False
    
    def generate_response(self, prompt, model=ollama_default_model, stream=False):
        """Generate response using Ollama API with GPU optimization"""
        url = f"{self.base_url}/api/generate"
        
        # Optimize for MX250's limited VRAM
        gpu_options = {
            "num_predict": 1000,      # Reasonable token limit
            "temperature": 0.7,
            "num_ctx": 2048,          # Context window size
            "num_gpu": 35,            # GPU layers for MX250
            "num_thread": 4,          # CPU threads
        }
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": gpu_options
        }
        
        # Adjust timeout based on model and GPU usage
        if "qwen" in model.lower():
            timeout = 120  # Reduced with GPU
        elif "gemma3:12b" in model.lower():
            timeout = 180  # Still large model
        else:
            timeout = 45   # Faster with GPU
        
        try:
            print(f"Generating with {model} on GPU (timeout: {timeout}s)...")
            response = requests.post(url, json=payload, timeout=timeout)
            response.raise_for_status()
            result = response.json()
            
            if 'response' in result:
                print(f"GPU generation completed: {len(result['response'])} chars")
                # Don't print full response to avoid clutter
            
            return result
            
        except requests.exceptions.Timeout:
            print(f"GPU timeout after {timeout} seconds with model {model}")
            return None
        except requests.exceptions.RequestException as e:
            print(f"GPU generation error: {e}")
            return None
````

## 4. Start Ollama with GPU Support

Modify your setup function to enable GPU:

````python
def setup_and_start():
    """Setup Ollama with GPU acceleration and start the Flask server"""
    print("Setting up Ollama with NVIDIA MX250 GPU acceleration...")
    
    # Setup GPU environment
    import os
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    os.environ['OLLAMA_GPU_LAYERS'] = '35'
    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'
    
    # Install Ollama
    if not ollama_manager.install_ollama():
        print("‚ö†Ô∏è  Please install Ollama manually")
        print("Then run: ollama serve")
        return None
    
    # Check GPU acceleration
    print("\nüîç Checking GPU acceleration...")
    gpu_working = ollama_manager.check_gpu_acceleration()
    
    if gpu_working:
        print("‚úÖ NVIDIA MX250 GPU acceleration is working!")
    else:
        print("‚ö†Ô∏è  GPU acceleration may not be working. Check CUDA installation.")
    
    # Pull optimized models for MX250
    recommended_models = ["gemma3:1b", "gemma3:4b"]  # Start with smaller models
    for model in recommended_models:
        print(f"\nüì• Pulling {model} (optimized for MX250)...")
        ollama_manager.pull_gemma_model(model)
    
    # List available models
    models = ollama_manager.list_models()
    if models:
        print("\nüìã Available models:")
        print(models)
    
    # Start Flask server
    start_server()
    
    print(f"\n‚úÖ Server setup complete with GPU acceleration!")
    print(f"üåê Health check: http://localhost:5000/health")
    print(f"üéÆ GPU Status: {'Enabled' if gpu_working else 'Check CUDA installation'}")
````

## 5. Monitor GPU Usage

Add GPU monitoring to your health check:

````python
@app.route('/health', methods=['GET'])
def health_check():
    """Enhanced health check with GPU monitoring"""
    try:
        # Check Ollama connectivity
        ollama_status = "unknown"
        gpu_status = "unknown"
        
        try:
            models = ollama_manager.list_models()
            ollama_status = "healthy" if models else "unhealthy"
            
            # Check GPU usage
            gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', 
                                       '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True)
            if gpu_result.returncode == 0:
                gpu_info = gpu_result.stdout.strip().split(', ')
                gpu_status = f"GPU: {gpu_info[0]}% util, {gpu_info[1]}MB/{gpu_info[2]}MB memory"
            else:
                gpu_status = "GPU not available"
                
        except Exception as e:
            gpu_status = f"GPU check failed: {e}"
        
        # Check database connectivity
        db_status = "unknown"
        try:
            conn = sqlite3.connect(api_key_manager.db_path)
            conn.close()
            db_status = "healthy"
        except:
            db_status = "unhealthy"
        
        overall_status = "healthy" if ollama_status == "healthy" and db_status == "healthy" else "degraded"
        
        return jsonify({
            'status': overall_status,
            'timestamp': datetime.utcnow().isoformat(),
            'services': {
                'ollama': ollama_status,
                'database': db_status,
                'gpu': gpu_status
            }
        })
    except Exception as e:
        return jsonify({
            'status': 'unhealthy',
            'timestamp': datetime.utcnow().isoformat(),
            'error': str(e)
        }), 500
````

## 6. Test GPU Acceleration

Add a test function to verify GPU is working:

````python
def test_gpu_performance():
    """Test GPU vs CPU performance"""
    import time
    
    test_prompt = "Explain quantum computing in simple terms."
    model = "gemma3:1b"  # Use small model for testing
    
    print(f"Testing {model} performance...")
    
    # Test with GPU
    start_time = time.time()
    gpu_response = ollama_manager.generate_response(test_prompt, model)
    gpu_time = time.time() - start_time
    
    print(f"\n‚è±Ô∏è  GPU Generation Time: {gpu_time:.2f} seconds")
    if gpu_response and 'response' in gpu_response:
        print(f"üìù Response length: {len(gpu_response['response'])} characters")
    
    return gpu_time

# Run the test
test_gpu_performance()
````

## Important Notes for MX250:

1. **Limited VRAM**: MX250 has only 2GB VRAM, so use smaller models (1B-4B parameters)
2. **Power Efficiency**: MX250 is designed for laptops, so it may throttle under heavy load
3. **Model Recommendations**: Start with `gemma3:1b` and `gemma3:4b` before trying larger models
4. **Memory Management**: The GPU memory limit settings are crucial to avoid crashes

The MX250 should provide modest acceleration compared to CPU-only inference, especially for smaller models. Monitor GPU utilization to ensure it's being used effectively.

---

_**User**_

is physX anything to do with GPU in running Ollama

---

_**Assistant**_

No, **PhysX has no direct relation to running Ollama or LLM inference**. Let me explain the difference:

## What is PhysX?

PhysX is NVIDIA's **physics simulation library** used primarily for:
- **Gaming**: Realistic physics effects (cloth, fluid, destruction, particle systems)
- **3D Graphics**: Real-time physics calculations in games and simulations
- **Visual Effects**: Movie/animation industry physics simulations

## What Ollama Uses for GPU Acceleration

Ollama uses **CUDA** (Compute Unified Device Architecture) for GPU acceleration:
- **CUDA Cores**: Parallel processing units for mathematical computations
- **CUDA Toolkit**: Software framework for general-purpose GPU computing
- **cuDNN**: Deep learning primitives optimized for neural networks

## Check What Your System Actually Needs

````python
def check_gpu_capabilities():
    """Check GPU capabilities for Ollama"""
    try:
        # Check NVIDIA GPU and CUDA
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        if result.returncode == 0:
            print("NVIDIA GPU detected:")
            print(result.stdout)
            
            # Check CUDA version
            try:
                cuda_result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
                if cuda_result.returncode == 0:
                    print("\nCUDA Toolkit found:")
                    print(cuda_result.stdout)
                else:
                    print("\n‚ùå CUDA Toolkit not installed")
                    print("Download from: https://developer.nvidia.com/cuda-toolkit")
            except FileNotFoundError:
                print("\n‚ùå CUDA Toolkit not found")
        else:
            print("‚ùå NVIDIA GPU not detected")
            
        # PhysX check (not needed for Ollama)
        print("\nüìã PhysX Status:")
        print("PhysX is NOT required for Ollama LLM inference")
        print("PhysX is only used for physics simulations in games/graphics")
        
    except FileNotFoundError:
        print("‚ùå NVIDIA drivers not installed")

check_gpu_capabilities()
````

## For Your MX250 GPU Setup

Your NVIDIA MX250 needs:

1. **NVIDIA Drivers** ‚úÖ
2. **CUDA Toolkit** ‚úÖ (for Ollama acceleration)
3. **PhysX** ‚ùå (not needed for Ollama)

## Update Your OllamaManager for MX250

````python
class OllamaManager:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.setup_cuda_environment()  # Changed from GPU to CUDA
        
    def setup_cuda_environment(self):
        """Setup CUDA environment for MX250 (not PhysX)"""
        import os
        
        # CUDA settings for MX250
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        os.environ['OLLAMA_GPU_LAYERS'] = '20'    # Conservative for MX250
        os.environ['OLLAMA_GPU_MEMORY'] = '1.5GB' # MX250 has 2GB VRAM
        
        print("CUDA environment configured for NVIDIA MX250")
        print("PhysX is NOT used for LLM inference")
    
    def check_cuda_acceleration(self):
        """Check CUDA acceleration (not PhysX)"""
        try:
            # Test CUDA availability
            response = requests.get(f"{self.base_url}/api/ps")
            if response.status_code == 200:
                processes = response.json()
                
                # Look for GPU usage in processes
                gpu_in_use = any('gpu' in str(proc).lower() for proc in processes.get('models', []))
                
                if gpu_in_use:
                    print("‚úÖ CUDA GPU acceleration detected")
                    return True
                else:
                    print("‚ö†Ô∏è  Running on CPU only")
                    return False
            
        except Exception as e:
            print(f"Error checking CUDA: {e}")
            return False
````

## Summary

- **PhysX**: Physics simulations for games ‚ùå (not for Ollama)
- **CUDA**: Parallel computing for AI/ML ‚úÖ (needed for Ollama)
- **Your MX250**: Supports CUDA, not powerful but can help with smaller models

Focus on installing **CUDA Toolkit**, not PhysX, for Ollama GPU acceleration.

---

_**User**_

what if we load llm model like gemma3:4b which has 3.3GB size, can we utilize the 8GB shared memory for MX250 to load the model?

---

_**Assistant**_

Great question! Yes, **theoretically you can utilize the 8GB shared memory for loading larger models** like `gemma3:4b` with the MX250, but there are important caveats to understand.

## How MX250 Memory Works

The MX250 has:
- **2GB dedicated VRAM** (fast GPU memory)
- **Access to up to 8GB shared system RAM** (slower, shared with CPU)

## Update Your GPU Performance Test

Let's modify your test function to monitor memory usage with larger models:

````python
def test_gpu_performance_with_memory_monitoring():
    """Test GPU performance with memory monitoring for larger models"""
    import time
    import psutil
    
    test_prompt = "Explain quantum computing in simple terms."
    models_to_test = [
        "gemma3:1b",   # ~1.3GB - fits in VRAM
        "gemma3:4b",   # ~3.3GB - needs shared memory
    ]
    
    for model in models_to_test:
        print(f"\n{'='*50}")
        print(f"Testing {model} performance...")
        print(f"{'='*50}")
        
        # Check memory before
        memory_before = psutil.virtual_memory()
        print(f"üìä System RAM before: {memory_before.used / (1024**3):.1f}GB used / {memory_before.total / (1024**3):.1f}GB total")
        
        # Check GPU memory before (if nvidia-smi available)
        try:
            gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', 
                                       '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True)
            if gpu_result.returncode == 0:
                gpu_mem = gpu_result.stdout.strip().split(', ')
                print(f"üéÆ GPU memory before: {gpu_mem[0]}MB / {gpu_mem[1]}MB")
        except:
            print("üéÆ GPU memory check not available")
        
        # Test generation
        start_time = time.time()
        response = ollama_manager.generate_response(test_prompt, model)
        generation_time = time.time() - start_time
        
        print(f"\n‚è±Ô∏è  Generation Time: {generation_time:.2f} seconds")
        if response and 'response' in response:
            print(f"üìù Response length: {len(response['response'])} characters")
            
        # Check memory after
        memory_after = psutil.virtual_memory()
        memory_diff = (memory_after.used - memory_before.used) / (1024**3)
        print(f"üìä System RAM after: {memory_after.used / (1024**3):.1f}GB used (diff: {memory_diff:+.1f}GB)")
        
        # Check GPU memory after
        try:
            gpu_result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,memory.total', 
                                       '--format=csv,noheader,nounits'], 
                                      capture_output=True, text=True)
            if gpu_result.returncode == 0:
                gpu_mem_after = gpu_result.stdout.strip().split(', ')
                print(f"üéÆ GPU memory after: {gpu_mem_after[0]}MB / {gpu_mem_after[1]}MB")
        except:
            pass
        
        print(f"‚úÖ {model} test completed")
        
        # Small delay between tests
        time.sleep(2)

# Run the enhanced test
test_gpu_performance_with_memory_monitoring()
````

## Configure Ollama for Shared Memory Usage

Update your OllamaManager to better utilize shared memory:

````python
class OllamaManager:
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
        self.setup_mx250_environment()
        
    def setup_mx250_environment(self):
        """Setup environment optimized for MX250 with shared memory"""
        import os
        
        # MX250 with shared memory configuration
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
        
        # Allow Ollama to use both VRAM and shared memory
        os.environ['OLLAMA_GPU_LAYERS'] = '20'        # Conservative layer count
        os.environ['OLLAMA_GPU_MEMORY'] = '6GB'       # Use up to 6GB (2GB VRAM + 4GB shared)
        
        # Performance optimization for shared memory
        os.environ['OLLAMA_NUM_PARALLEL'] = '1'       # Avoid memory fragmentation
        os.environ['OLLAMA_MAX_LOADED_MODELS'] = '1'  # Only load one model at a time
        
        print("üéÆ MX250 environment configured for shared memory usage")
        print("üìù Configuration:")
        print(f"   - GPU Layers: {os.environ.get('OLLAMA_GPU_LAYERS')}")
        print(f"   - GPU Memory: {os.environ.get('OLLAMA_GPU_MEMORY')}")
        print(f"   - Max Models: {os.environ.get('OLLAMA_MAX_LOADED_MODELS')}")

    def generate_response_with_memory_optimization(self, prompt, model="gemma3:4b", stream=False):
        """Generate response with MX250 memory optimization"""
        url = f"{self.base_url}/api/generate"
        
        # Model-specific optimization
        if "gemma3:4b" in model.lower():
            # For 3.3GB model on MX250
            gpu_options = {
                "num_predict": 2000,      # Reasonable token limit
                "temperature": 0.7,
                "num_ctx": 4096,          # Context window
                "num_gpu": 20,            # Conservative GPU layers for shared memory
                "num_thread": 2,          # Fewer threads to avoid memory pressure
                "numa": False,            # Disable NUMA for consistent memory access
            }
            timeout = 120
        elif "gemma3:1b" in model.lower():
            # For 1.3GB model (fits in VRAM)
            gpu_options = {
                "num_predict": 2000,
                "temperature": 0.7,
                "num_ctx": 4096,
                "num_gpu": 35,            # More layers can fit in VRAM
                "num_thread": 4,
            }
            timeout = 60
        else:
            # Default settings
            gpu_options = {
                "num_predict": 1500,
                "temperature": 0.7,
                "num_ctx": 2048,
                "num_gpu": 15,
                "num_thread": 2,
            }
            timeout = 90
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "options": gpu_options
        }
        
        try:
            print(f"üöÄ Generating with {model} using shared memory (timeout: {timeout}s)...")
            response = requests.post(url, json=payload, timeout=timeout)
            response.raise_for_status()
            result = response.json()
            
            if 'response' in result:
                print(f"‚úÖ Generation completed: {len(result['response'])} chars")
                print(f"üì± Model loaded in: {'VRAM+Shared' if 'gemma3:4b' in model else 'VRAM'} memory")
            
            return result
            
        except requests.exceptions.Timeout:
            print(f"‚è∞ Timeout after {timeout} seconds - model may be too large for available memory")
            return None
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Generation error: {e}")
            return None

# Update the manager with new method
ollama_manager.generate_response = ollama_manager.generate_response_with_memory_optimization.__get__(ollama_manager, OllamaManager)
````

## Check Model Loading in Shared Memory

Add a function to verify how models are loaded:

````python
def check_model_memory_usage():
    """Check how models are using memory"""
    try:
        # Check which models are currently loaded
        response = requests.get('http://localhost:11434/api/ps')
        if response.status_code == 200:
            loaded_models = response.json()
            print("üìã Currently loaded models:")
            print(json.dumps(loaded_models, indent=2))
            
            # Analyze memory distribution
            if loaded_models.get('models'):
                for model_info in loaded_models['models']:
                    model_name = model_info.get('name', 'unknown')
                    size = model_info.get('size', 0)
                    print(f"\nüìä Model: {model_name}")
                    print(f"   Size: {size / (1024**3):.1f}GB")
                    
                    # Estimate memory distribution
                    if size > 2 * 1024**3:  # > 2GB
                        print(f"   üéÆ Memory: VRAM (2GB) + Shared RAM ({(size - 2*1024**3) / 1024**3:.1f}GB)")
                    else:
                        print(f"   üéÆ Memory: VRAM ({size / 1024**3:.1f}GB)")
        else:
            print("‚ùå Could not check loaded models")
            
    except Exception as e:
        print(f"‚ùå Error checking memory usage: {e}")

# Test loading gemma3:4b
print("Testing gemma3:4b model loading...")
response = ollama_manager.generate_response("Hello", model="gemma3:4b")
check_model_memory_usage()
````

## Performance Expectations

Here's what to expect with `gemma3:4b` on MX250:

| Memory Type | Speed | Capacity | Usage for gemma3:4b |
|-------------|--------|----------|-------------------|
| **VRAM** | Fast | 2GB | First 2GB of model |
| **Shared RAM** | Slower | 6GB available | Remaining 1.3GB of model |
| **Total** | Mixed | 8GB | Full 3.3GB model fits |

## Key Considerations

1. **Performance Impact**: Shared memory is **significantly slower** than VRAM
2. **Generation Speed**: Expect 2-3x slower inference compared to models that fit entirely in VRAM
3. **Memory Pressure**: May affect system performance when other applications need RAM
4. **Stability**: Shared memory usage can be less stable than dedicated VRAM

## Optimized Model Recommendations for MX250

````python
def get_mx250_optimized_models():
    """Get model recommendations optimized for MX250"""
    recommendations = {
        "fits_in_vram": {
            "models": ["gemma3:1b", "qwen2:1.5b"],
            "performance": "Fast",
            "memory": "2GB VRAM only",
            "recommended": True
        },
        "uses_shared_memory": {
            "models": ["gemma3:4b", "llama3.1:3b"],
            "performance": "Moderate (2-3x slower)",
            "memory": "2GB VRAM + 2-4GB Shared",
            "recommended": "For better quality when speed isn't critical"
        },
        "too_large": {
            "models": ["gemma3:12b", "llama3.1:8b"],
            "performance": "Slow/Unstable",
            "memory": "> 8GB total",
            "recommended": False
        }
    }
    
    for category, info in recommendations.items():
        print(f"\n{category.upper().replace('_', ' ')}:")
        print(f"  Models: {', '.join(info['models'])}")
        print(f"  Performance: {info['performance']}")
        print(f"  Memory: {info['memory']}")
        print(f"  Recommended: {info['recommended']}")
    
    return recommendations

get_mx250_optimized_models()
````

## Summary

**Yes, you can load `gemma3:4b` (3.3GB) using MX250's shared memory access**, but expect:

- ‚úÖ **Works**: Model will load and run
- ‚ö†Ô∏è **Slower**: 2-3x slower than VRAM-only models
- ‚ö†Ô∏è **System Impact**: Uses significant system RAM
- ‚úÖ **Better Quality**: Better responses than smaller models

**Best Practice**: Use `gemma3:1b` for speed, `gemma3:4b` for quality when you can tolerate slower responses.

---

_**User**_

Is it possible to free system memory RAM from python?

---

_**Assistant**_

Yes, there are several ways to free system memory RAM from Python. Here are the most effective methods:

## 1. **Garbage Collection**

Force Python's garbage collector to run immediately:

````python
import gc

def force_garbage_collection():
    """Force immediate garbage collection to free memory"""
    # Collect all generations
    collected = gc.collect()
    print(f"Garbage collected {collected} objects")
    
    # Get memory stats before and after
    import psutil
    process = psutil.Process()
    memory_info = process.memory_info()
    print(f"Memory usage: {memory_info.rss / (1024**2):.1f} MB")

# Call when you want to free memory
force_garbage_collection()
````

## 2. **Delete Large Objects and Variables**

Explicitly delete variables holding large data:

````python
def cleanup_model_memory():
    """Clean up model-related memory"""
    # Delete large variables
    if 'large_model_data' in globals():
        del large_model_data
    
    if 'cached_responses' in globals():
        del cached_responses
    
    # Clear any caches
    import functools
    functools.lru_cache.cache_clear() if hasattr(functools.lru_cache, 'cache_clear') else None
    
    # Force garbage collection
    gc.collect()
    print("‚úÖ Model memory cleaned up")

# Example: Clean up after model switching
def switch_model_with_cleanup(new_model):
    """Switch models with memory cleanup"""
    # Unload current model (if API supports it)
    try:
        requests.post('http://localhost:11434/api/unload', 
                     json={"model": "current_model"})
    except:
        pass
    
    # Clean up Python memory
    cleanup_model_memory()
    
    # Load new model
    print(f"Loading {new_model}...")
    response = ollama_manager.generate_response("Hello", model=new_model)
    return response
````

## 3. **Memory Management for Ollama Models**

Add memory management specifically for your LLM setup:

````python
class MemoryManager:
    def __init__(self):
        self.loaded_models = set()
        
    def check_memory_usage(self):
        """Check current memory usage"""
        import psutil
        
        # System memory
        memory = psutil.virtual_memory()
        print(f"üñ•Ô∏è  System RAM: {memory.used / (1024**3):.1f}GB / {memory.total / (1024**3):.1f}GB ({memory.percent}%

---

